{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here’s the first block, focusing on data acquisition.\n",
    "\n",
    "``` python\n",
    "# 1. data acquisition:\n",
    "import pandas as pd\n",
    "import requests\n",
    "from io import StringIO\n",
    "import numpy as np # Will be needed later, good to import early\n",
    "\n",
    "# URLs\n",
    "train_url     = \"https://raw.githubusercontent.com/defcom17/NSL_KDD/master/KDDTrain%2B.txt\"\n",
    "test_url      = \"https://raw.githubusercontent.com/defcom17/NSL_KDD/master/KDDTest%2B.txt\"\n",
    "features_url = \"https://raw.githubusercontent.com/defcom17/NSL_KDD/master/Field%20Names.csv\"\n",
    "\n",
    "# Define column names based on the Field Names.csv structure\n",
    "column_names = []\n",
    "try:\n",
    "    print(\"Attempting to fetch feature names...\")\n",
    "    features_response = requests.get(features_url)\n",
    "    features_response.raise_for_status()  # Check for HTTP errors\n",
    "    # Read the CSV, it has 'feature_name,type'. We only need the names.\n",
    "    features_df = pd.read_csv(StringIO(features_response.text), header=None, names=['feature_name', 'type'])\n",
    "    column_names = features_df['feature_name'].tolist()\n",
    "    # Add the target and difficulty columns which are not in Field Names.csv\n",
    "    column_names.extend(['attack_type', 'difficulty_level'])\n",
    "    print(\"Feature names fetched successfully.\")\n",
    "except requests.exceptions.RequestException as e:\n",
    "    print(f\"Error fetching feature names: {e}\")\n",
    "    # Fallback list if fetch fails (ensures notebook can run partially)\n",
    "    column_names = [\n",
    "        'duration', 'protocol_type', 'service', 'flag', 'src_bytes', 'dst_bytes', 'land',\n",
    "        'wrong_fragment', 'urgent', 'hot', 'num_failed_logins', 'logged_in', 'num_compromised',\n",
    "        'root_shell', 'su_attempted', 'num_root', 'num_file_creations', 'num_shells',\n",
    "        'num_access_files', 'num_outbound_cmds', 'is_host_login', 'is_guest_login',\n",
    "        'count', 'srv_count', 'serror_rate', 'srv_serror_rate', 'rerror_rate',\n",
    "        'srv_rerror_rate', 'same_srv_rate', 'diff_srv_rate', 'srv_diff_host_rate',\n",
    "        'dst_host_count', 'dst_host_srv_count', 'dst_host_same_srv_rate',\n",
    "        'dst_host_diff_srv_rate', 'dst_host_same_src_port_rate',\n",
    "        'dst_host_srv_diff_host_rate', 'dst_host_serror_rate',\n",
    "        'dst_host_srv_serror_rate', 'dst_host_rerror_rate', 'dst_host_srv_rerror_rate',\n",
    "        'attack_type', 'difficulty_level']\n",
    "    print(\"Using fallback feature names.\")\n",
    "\n",
    "# Fetch datasets\n",
    "df_train = None\n",
    "df_test = None\n",
    "data_loaded = False\n",
    "try:\n",
    "    print(f\"\\nFetching training data from {train_url}...\")\n",
    "    train_response = requests.get(train_url)\n",
    "    train_response.raise_for_status()\n",
    "    df_train = pd.read_csv(StringIO(train_response.text), header=None, names=column_names)\n",
    "    print(\"Training data loaded successfully.\")\n",
    "\n",
    "    print(f\"Fetching testing data from {test_url}...\")\n",
    "    test_response = requests.get(test_url)\n",
    "    test_response.raise_for_status()\n",
    "    df_test = pd.read_csv(StringIO(test_response.text), header=None, names=column_names)\n",
    "    print(\"Testing data loaded successfully.\")\n",
    "    data_loaded = True\n",
    "\n",
    "    print(\"\\nTraining data shape:\", df_train.shape)\n",
    "    print(\"Testing data shape:\", df_test.shape)\n",
    "\n",
    "    # Drop the 'difficulty_level' column as it's not typically used as a feature for detection model training\n",
    "    if 'difficulty_level' in df_train.columns:\n",
    "        df_train = df_train.drop('difficulty_level', axis=1)\n",
    "    if 'difficulty_level' in df_test.columns:\n",
    "        df_test = df_test.drop('difficulty_level', axis=1)\n",
    "    print(\"\\nDropped 'difficulty_level' column.\")\n",
    "\n",
    "    # Create binary target variable: 1 for attack, 0 for normal\n",
    "    # This is crucial for binary classification focused on intrusion detection\n",
    "    df_train['is_attack'] = (df_train['attack_type'] != 'normal').astype(int)\n",
    "    df_test['is_attack'] = (df_test['attack_type'] != 'normal').astype(int)\n",
    "    print(\"Created binary target 'is_attack'.\")\n",
    "\n",
    "    # Drop the original 'attack_type' column as 'is_attack' is now the target\n",
    "    df_train = df_train.drop('attack_type', axis=1)\n",
    "    df_test = df_test.drop('attack_type', axis=1)\n",
    "    print(\"Dropped original 'attack_type' column.\")\n",
    "\n",
    "    print(\"\\nTraining data shape after transformations:\", df_train.shape)\n",
    "    print(\"Testing data shape after transformations:\", df_test.shape)\n",
    "\n",
    "    print(\"\\nTraining data head:\")\n",
    "    print(df_train.head())\n",
    "\n",
    "except requests.exceptions.RequestException as e:\n",
    "    print(f\"Error fetching data: {e}\")\n",
    "    print(\"Please ensure you have an internet connection and the URLs are correct.\")\n",
    "    print(\"Cannot proceed without data files.\")\n",
    "except Exception as e:\n",
    "    print(f\"An unexpected error occurred during data loading or initial processing: {e}\")\n",
    "finally:\n",
    "    if not data_loaded:\n",
    "        print(\"\\nData loading failed. Initializing empty data structures for notebook structure demonstration.\")\n",
    "        # Use the column_names defined earlier, excluding the dropped ones and target\n",
    "        feature_cols = [col for col in column_names if col not in ['attack_type', 'difficulty_level', 'is_attack']]\n",
    "        df_train = pd.DataFrame(columns=feature_cols + ['is_attack'])\n",
    "        df_test = pd.DataFrame(columns=feature_cols + ['is_attack'])\n",
    "        # To allow subsequent cells to run without erroring on .shape or .columns\n",
    "        X_train_raw, y_train = pd.DataFrame(columns=feature_cols), pd.Series(dtype='int', name='is_attack')\n",
    "        X_test_raw, y_test = pd.DataFrame(columns=feature_cols), pd.Series(dtype='int', name='is_attack')\n",
    "        X_train_p, X_test_p = np.array([]).reshape(0, len(feature_cols)), np.array([]).reshape(0, len(feature_cols))\n",
    "        X_tr, X_val, y_tr, y_val = np.array([]).reshape(0, len(feature_cols)), np.array([]).reshape(0, len(feature_cols)), pd.Series(dtype='int', name='is_attack'), pd.Series(dtype='int', name='is_attack')\n",
    "        feat_names = feature_cols\n",
    "```\n",
    "\n",
    "**Rationale for Step 1 Modifications:** 1. **Imports:** Added `numpy`\n",
    "import as it’s generally useful and will be needed. 2. **Error\n",
    "Handling:** Ensured `raise_for_status()` is used for both feature names\n",
    "and data fetching to catch HTTP errors. 3. **Column Names from CSV:**\n",
    "Explicitly named columns in `pd.read_csv` for `features_df` as\n",
    "`['feature_name', 'type']` for clarity and robustness, then selected\n",
    "`feature_name`. 4. **Target Variable:** The creation of `is_attack` and\n",
    "dropping `attack_type` is standard for binary intrusion detection.\n",
    "Clarified comments. 5. **`finally` block for empty structures:**\n",
    "Modified the initialization of empty DataFrames and Series in the\n",
    "`finally` block to ensure they have the correct target column name\n",
    "(`is_attack`) and that `X_train_raw`, `y_train` etc. are also\n",
    "initialized as DataFrames/Series to prevent downstream errors if data\n",
    "loading fails. This makes the notebook more robust for partial runs or\n",
    "demonstrations. 6. **Print Statements:** Added more descriptive print\n",
    "statements for better tracking of the data loading process.\n",
    "\n",
    "This block remains largely similar to your original, as it was already\n",
    "well-structured for data acquisition. The changes primarily enhance\n",
    "robustness and clarity.\n",
    "\n",
    "``` python\n",
    "# 2. Data preprocessing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler, OneHotEncoder, StandardScaler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "if data_loaded and not df_train.empty:\n",
    "    # Split features and target variable\n",
    "    X_train_raw = df_train.drop('is_attack', axis=1)\n",
    "    y_train = df_train['is_attack']\n",
    "    X_test_raw = df_test.drop('is_attack', axis=1)\n",
    "    y_test = df_test['is_attack']\n",
    "\n",
    "    # Identify categorical vs numerical features\n",
    "    # Ensure we only select columns that exist in the dataframe\n",
    "    all_features = X_train_raw.columns.tolist()\n",
    "    \n",
    "    # Explicitly define categorical features based on NSL-KDD dataset knowledge\n",
    "    cat_feats = ['protocol_type', 'service', 'flag']\n",
    "    # Ensure these are actually present in the columns, in case of fallback names or modifications\n",
    "    cat_feats = [col for col in cat_feats if col in all_features]\n",
    "    \n",
    "    num_feats = [col for col in all_features if col not in cat_feats]\n",
    "\n",
    "    print(f\"\\nIdentified {len(cat_feats)} categorical features: {cat_feats}\")\n",
    "    print(f\"Identified {len(num_feats)} numerical features: {num_feats}\")\n",
    "\n",
    "    # Define preprocessing pipelines for numerical and categorical features\n",
    "    # Numerical features: Impute missing values with median, then scale.\n",
    "    # StandardScaler is often preferred for algorithms sensitive to feature distribution (e.g., SVM, NN).\n",
    "    # MinMaxScaler is also a valid choice, especially if features have varying scales but no specific distribution is assumed.\n",
    "    # Let's stick to MinMaxScaler as per original, but StandardScaler is a good alternative to consider.\n",
    "    num_pipe = Pipeline([\n",
    "        ('impute', SimpleImputer(strategy='median')),\n",
    "        ('scale', MinMaxScaler()) # Or StandardScaler()\n",
    "    ])\n",
    "\n",
    "    # Categorical features: Impute missing values with a constant, then one-hot encode.\n",
    "    # handle_unknown='ignore' ensures that if new categories appear in test data, they are handled gracefully.\n",
    "    cat_pipe = Pipeline([\n",
    "        ('impute', SimpleImputer(strategy='constant', fill_value='missing')),\n",
    "        ('onehot', OneHotEncoder(handle_unknown='ignore', sparse_output=False)) # sparse_output=False for dense array\n",
    "    ])\n",
    "\n",
    "    # Create a ColumnTransformer to apply different transformations to different columns\n",
    "    # remainder='passthrough' keeps any columns not explicitly transformed (should be none here if lists are correct)\n",
    "    preprocessor = ColumnTransformer([\n",
    "        ('num', num_pipe, num_feats),\n",
    "        ('cat', cat_pipe, cat_feats)\n",
    "    ], remainder='passthrough') # Changed variable name to 'preprocessor' for clarity\n",
    "\n",
    "    # Fit the preprocessor on the training data and transform both training and test data\n",
    "    # Fitting only on training data prevents data leakage from the test set.\n",
    "    print(\"\\nFitting preprocessor on training data...\")\n",
    "    X_train_p = preprocessor.fit_transform(X_train_raw)\n",
    "    print(\"Transforming test data...\")\n",
    "    X_test_p = preprocessor.transform(X_test_raw)\n",
    "\n",
    "    # Get feature names after preprocessing (important for interpretability)\n",
    "    try:\n",
    "        # For scikit-learn >= 1.0\n",
    "        feat_names = preprocessor.get_feature_names_out()\n",
    "    except AttributeError:\n",
    "        # Fallback for older scikit-learn versions\n",
    "        cat_transformed_names = preprocessor.named_transformers_['cat']['onehot'].get_feature_names_out(cat_feats).tolist()\n",
    "        feat_names = num_feats + cat_transformed_names\n",
    "        # Handle remainder='passthrough' if used and columns were passed through\n",
    "        if preprocessor.remainder == 'passthrough' and hasattr(preprocessor, '_columns') and preprocessor._columns:\n",
    "             # This part is tricky with older sklearn versions and remainder='passthrough'\n",
    "             # For simplicity, assuming no passthrough columns if get_feature_names_out fails\n",
    "            pass\n",
    "\n",
    "    feat_names = list(feat_names) # Ensure it's a list\n",
    "\n",
    "    print(f\"\\nProcessed training data shape: {X_train_p.shape}\")\n",
    "    print(f\"Processed testing data shape: {X_test_p.shape}\")\n",
    "    print(f\"Number of features after preprocessing: {len(feat_names)}\")\n",
    "\n",
    "    # Create an internal train/validation split from the processed training data\n",
    "    # This validation set (X_val, y_val) is used by the feature selection algorithms\n",
    "    # Stratify by y_train to maintain class proportions, crucial for imbalanced datasets\n",
    "    X_tr, X_val, y_tr, y_val = train_test_split(\n",
    "        X_train_p, y_train, test_size=0.25, stratify=y_train, random_state=42\n",
    "    )\n",
    "    print(f\"Internal training set shape for FS: X_tr {X_tr.shape}, y_tr {y_tr.shape}\")\n",
    "    print(f\"Internal validation set shape for FS: X_val {X_val.shape}, y_val {y_val.shape}\")\n",
    "\n",
    "else:\n",
    "    print(\"\\nSkipping preprocessing as data was not loaded or df_train is empty.\")\n",
    "    # Ensure these variables exist as empty structures if data loading failed, to prevent downstream errors\n",
    "    if 'X_train_raw' not in locals(): X_train_raw = pd.DataFrame()\n",
    "    if 'y_train' not in locals(): y_train = pd.Series(dtype='int', name='is_attack')\n",
    "    if 'X_test_raw' not in locals(): X_test_raw = pd.DataFrame()\n",
    "    if 'y_test' not in locals(): y_test = pd.Series(dtype='int', name='is_attack')\n",
    "    if 'X_train_p' not in locals(): X_train_p = np.array([]).reshape(0,0)\n",
    "    if 'X_test_p' not in locals(): X_test_p = np.array([]).reshape(0,0)\n",
    "    if 'X_tr' not in locals(): X_tr, X_val, y_tr, y_val = np.array([]).reshape(0,0), np.array([]).reshape(0,0), pd.Series(dtype='int', name='is_attack'), pd.Series(dtype='int', name='is_attack')\n",
    "    if 'feat_names' not in locals(): feat_names = []\n",
    "```\n",
    "\n",
    "**Rationale for Step 2 Modifications:** 1. **Variable Naming:** Changed\n",
    "`pre` to `preprocessor` for better readability. 2. **Categorical\n",
    "Features:** Explicitly defined `cat_feats` based on common knowledge of\n",
    "the NSL-KDD dataset (`protocol_type`, `service`, `flag`). Added a check\n",
    "to ensure these features are present in the loaded data. This is more\n",
    "robust than inferring them. 3. **`OneHotEncoder`:** Set\n",
    "`sparse_output=False` to get a dense numpy array directly from the\n",
    "`OneHotEncoder`, which is often easier to work with in subsequent steps,\n",
    "especially with swarm algorithms that expect dense arrays. 4. **Feature\n",
    "Names:** Simplified the `get_feature_names_out` logic. The `try-except`\n",
    "block for `get_feature_names_out` is good. The fallback for older\n",
    "scikit-learn versions was slightly adjusted for `get_feature_names_out`\n",
    "on the `OneHotEncoder` part. Ensured `feat_names` is a list. 5.\n",
    "**Stratification:** Emphasized the importance of `stratify=y_train` in\n",
    "`train_test_split` for imbalanced datasets like NSL-KDD. 6. **Print\n",
    "Statements:** Added more informative print statements to track the\n",
    "shapes and number of features at various stages. 7. **Empty Data\n",
    "Handling:** Improved the `else` block to ensure all necessary variables\n",
    "are initialized as empty but correctly typed structures if data loading\n",
    "failed, preventing errors in subsequent cells. Specifically, `y_train`,\n",
    "`y_test`, `y_tr`, `y_val` are initialized as `pd.Series` with\n",
    "`dtype='int'` and the correct name.\n",
    "\n",
    "The core preprocessing logic (pipelines, column transformer) was already\n",
    "sound. These changes focus on robustness, clarity, and best practices\n",
    "for this specific dataset.\n",
    "\n",
    "``` python\n",
    "# 3. Define fitness function\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "import numpy as np # Ensure numpy is imported\n",
    "\n",
    "# Global flag to ensure data_loaded check is effective\n",
    "# This should be true if step 1 and 2 completed successfully with data\n",
    "data_ready_for_fs = data_loaded and 'X_tr' in globals() and X_tr.shape[0] > 0\n",
    "\n",
    "if data_ready_for_fs:\n",
    "    def feature_fitness(mask, X_train_subset, y_train_subset, X_val_subset, y_val_subset,\n",
    "                        acc_weight=1.0, fpr_penalty_weight=0.5):\n",
    "        \"\"\"\n",
    "        Evaluates the fitness of a feature subset.\n",
    "        The goal is to maximize accuracy and minimize False Positive Rate (FPR).\n",
    "        Fitness = acc_weight * Accuracy - fpr_penalty_weight * FPR\n",
    "        A penalty for the number of features will be applied by the calling optimizer.\n",
    "        \"\"\"\n",
    "        selected_indices = np.where(mask == 1)[0]\n",
    "\n",
    "        # If no features are selected, return a very low fitness (or (0,1) for (acc, fpr))\n",
    "        if len(selected_indices) == 0:\n",
    "            return 0.0, 1.0  # 0 accuracy, 1.0 FPR (worst case for FPR)\n",
    "\n",
    "        # Select features from the data subsets\n",
    "        X_train_selected = X_train_subset[:, selected_indices]\n",
    "        X_val_selected = X_val_subset[:, selected_indices]\n",
    "\n",
    "        try:\n",
    "            # Check for sufficient samples for the classifier\n",
    "            if X_train_selected.shape[0] < 2 or X_val_selected.shape[0] < 1: # Need at least 1 sample in val for score\n",
    "                return 0.0, 1.0\n",
    "\n",
    "            # Use a fast and simple classifier for fitness evaluation\n",
    "            # Logistic Regression is a good choice.\n",
    "            # Parameters are set for speed and to avoid extensive tuning here.\n",
    "            # Increased max_iter slightly for potential convergence with small C.\n",
    "            clf = LogisticRegression(max_iter=250, solver='liblinear', random_state=42, C=0.1)\n",
    "            clf.fit(X_train_selected, y_train_subset)\n",
    "\n",
    "            # Evaluate on the validation set\n",
    "            y_pred_val = clf.predict(X_val_selected)\n",
    "            accuracy = accuracy_score(y_val_subset, y_pred_val)\n",
    "\n",
    "            # Calculate False Positive Rate (FPR)\n",
    "            # FPR = FP / (FP + TN)\n",
    "            cm = confusion_matrix(y_val_subset, y_pred_val)\n",
    "            if cm.shape == (2, 2):\n",
    "                TN, FP, FN, TP = cm.ravel()\n",
    "                if (FP + TN) == 0: # Avoid division by zero if no negatives or all are misclassified as positives\n",
    "                    fpr = 1.0 if FP > 0 else 0.0 # If FP > 0 and TN=0, then all negatives are FP.\n",
    "                else:\n",
    "                    fpr = FP / (FP + TN)\n",
    "            elif cm.shape == (1,1): # All samples belong to one class and are predicted as such\n",
    "                # This can happen if y_val_subset is all one class.\n",
    "                # If all are positive, TN=0, FP=0 -> FPR=0. If all are negative, TN=N, FP=0 -> FPR=0.\n",
    "                # However, if y_val_subset has only one class, FPR is ill-defined or 0.\n",
    "                # Let's assume if only one class in y_val, and it's predicted correctly, FPR is 0.\n",
    "                # If y_val_subset contains only positives, TN=0, FP=0.\n",
    "                # If y_val_subset contains only negatives, FP=0.\n",
    "                fpr = 0.0 # Default to 0 if confusion matrix is not 2x2 (e.g. all samples are of one class)\n",
    "            else: # Unexpected confusion matrix shape\n",
    "                fpr = 1.0 # Penalize if CM is not as expected\n",
    "\n",
    "            # Return accuracy and FPR. The calling function will combine them and add feature penalty.\n",
    "            if np.isnan(accuracy) or np.isnan(fpr):\n",
    "                return 0.0, 1.0 # Handle NaN cases\n",
    "            return accuracy, fpr\n",
    "\n",
    "        except ValueError as ve: # Catches errors like \"This solver needs samples of at least 2 classes\"\n",
    "            # print(f\"ValueError during fitness evaluation: {ve}\") # Uncomment for debugging\n",
    "            return 0.0, 1.0 # Low accuracy, high FPR on error\n",
    "        except Exception as e:\n",
    "            # print(f\"Error during fitness evaluation: {e}\") # Uncomment for debugging\n",
    "            return 0.0, 1.0 # Low accuracy, high FPR on error\n",
    "\n",
    "    print(\"\\nFeature selection fitness function (returning acc, fpr) defined.\")\n",
    "    # Example call to see its output structure (optional)\n",
    "    # dummy_mask = np.random.randint(0, 2, X_tr.shape[1])\n",
    "    # if np.sum(dummy_mask) > 0:\n",
    "    #     acc, fpr = feature_fitness(dummy_mask, X_tr, y_tr, X_val, y_val)\n",
    "    #     print(f\"Dummy fitness call: Accuracy={acc:.4f}, FPR={fpr:.4f}\")\n",
    "\n",
    "else:\n",
    "    print(\"\\nSkipping fitness function definition as data was not loaded or split correctly.\")\n",
    "    # Define a dummy function to avoid errors if called by subsequent cells\n",
    "    def feature_fitness(mask, X_train_subset, y_train_subset, X_val_subset, y_val_subset,\n",
    "                        acc_weight=1.0, fpr_penalty_weight=0.5):\n",
    "        print(\"Warning: Dummy fitness function called because data not loaded/prepared.\")\n",
    "        return 0.0, 1.0 # Returns (accuracy, fpr)\n",
    "```\n",
    "\n",
    "**Rationale for Step 3 Modifications:** 1. **Fitness Objective:** The\n",
    "core change is to modify the fitness function to help achieve the goal\n",
    "of higher accuracy and *lower false positives*. \\* The function\n",
    "`feature_fitness` now returns a tuple: `(accuracy, fpr)`. \\* The calling\n",
    "swarm optimization algorithm will be responsible for combining these\n",
    "metrics and applying a penalty for the number of selected features. This\n",
    "provides flexibility. 2. **FPR Calculation:** \\* The function now\n",
    "calculates the False Positive Rate (FPR) using `confusion_matrix` from\n",
    "`sklearn.metrics`. \\* `FPR = FP / (FP + TN)`. \\* Added handling for edge\n",
    "cases in FPR calculation (e.g., division by zero if `FP + TN == 0`, or\n",
    "if the confusion matrix isn’t 2x2). 3. **Parameters:** \\* The function\n",
    "signature was updated. `alpha` (which was for feature penalty) is\n",
    "removed from this function’s direct responsibility. \\* The `acc_weight`\n",
    "and `fpr_penalty_weight` are kept in the signature but are not used\n",
    "*within* this function anymore. They will be used by the caller. This\n",
    "was a change from my initial thought process to simplify\n",
    "`feature_fitness` itself. The swarm optimizers will now take these\n",
    "weights. 4. **Error Handling:** \\* Improved error handling within the\n",
    "`try-except` block. Specifically, it can catch `ValueError` which might\n",
    "occur if a subset of data passed to `LogisticRegression` has only one\n",
    "class. \\* Returns `(0.0, 1.0)` (low accuracy, high FPR) in case of\n",
    "errors, guiding the optimizer away from problematic feature subsets. 5.\n",
    "**Classifier:** `LogisticRegression` is kept as the evaluation\n",
    "classifier due to its speed. `max_iter` was slightly increased. `C=0.1`\n",
    "helps with regularization and speed. 6. **Data Check:** Added\n",
    "`data_ready_for_fs` flag for clarity on when this function should be\n",
    "defined. 7. **Return Values:** If no features are selected or if there’s\n",
    "an error, it returns `(0.0, 1.0)` to represent the worst possible\n",
    "outcome for accuracy and FPR.\n",
    "\n",
    "This revised fitness function provides the necessary components\n",
    "(accuracy and FPR) for the swarm algorithms to optimize towards the\n",
    "desired multi-objective goal. The actual combination of these metrics\n",
    "and the feature count penalty will now reside within each optimization\n",
    "algorithm’s main loop.\n",
    "\n",
    "``` python\n",
    "# 4. Individual algorithms\n",
    "import numpy as np\n",
    "import time\n",
    "from sklearn.linear_model import LogisticRegression # Already imported, but good for explicitness\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix # Already imported\n",
    "\n",
    "# === Common Utilities (modified for new fitness return) ===\n",
    "\n",
    "def _binarize_sigmoid(continuous_values, threshold=0.5):\n",
    "    \"\"\"Applies a sigmoid function and then thresholds to get binary values.\"\"\"\n",
    "    # Sigmoid function to map continuous values to (0, 1)\n",
    "    sigmoid_values = 1 / (1 + np.exp(-10 * (continuous_values - threshold))) # Scaled sigmoid\n",
    "    # Threshold to get binary 0 or 1\n",
    "    return (sigmoid_values > threshold).astype(int)\n",
    "\n",
    "def _calculate_composite_fitness(accuracy, fpr, num_selected, total_features,\n",
    "                                 acc_weight, fpr_weight, feat_penalty_weight):\n",
    "    \"\"\"Calculates the composite fitness score.\"\"\"\n",
    "    feature_ratio = num_selected / total_features if total_features > 0 else 0\n",
    "    fitness = (acc_weight * accuracy) - \\\n",
    "              (fpr_weight * fpr) - \\\n",
    "              (feat_penalty_weight * feature_ratio)\n",
    "    return fitness\n",
    "\n",
    "def _log_progress(algorithm_name, iteration, current_best_fitness, start_time, verbose_level):\n",
    "    \"\"\"Logs progress of the optimization algorithm.\"\"\"\n",
    "    # verbose_level: 0 = silent, 1 = summary, 2 = detailed\n",
    "    if verbose_level > 0 and (iteration % 5 == 0 or iteration == 0 or verbose_level > 1):\n",
    "        elapsed_time = time.time() - start_time\n",
    "        print(f\"[{algorithm_name}] Iter {iteration+1}: Best Fitness = {current_best_fitness:.5f}, Elapsed = {elapsed_time:.2f}s\")\n",
    "\n",
    "def _check_early_stopping(current_best_fitness, last_best_fitness, stall_counter, patience):\n",
    "    \"\"\"Checks for early stopping condition.\"\"\"\n",
    "    if current_best_fitness > last_best_fitness:\n",
    "        return 0, current_best_fitness  # Reset stall counter, update last_best_fitness\n",
    "    stall_counter += 1\n",
    "    return stall_counter, last_best_fitness\n",
    "\n",
    "# === Individual Swarm Methods (Updated for new fitness calculation) ===\n",
    "\n",
    "# Common parameters for all FS algorithms\n",
    "# These weights will determine the emphasis on accuracy, FPR, and feature count.\n",
    "# For example, higher fpr_weight means stronger penalty for high FPR.\n",
    "ACC_WEIGHT = 1.0\n",
    "FPR_WEIGHT = 0.7 # Increased emphasis on reducing FPR\n",
    "FEAT_PENALTY_WEIGHT = 0.02 # Small penalty for number of features\n",
    "\n",
    "def aco_fs(X_tr_fs, y_tr_fs, X_val_fs, y_val_fs,\n",
    "           n_ants=20, max_iter=30, evaporation_rate=0.1, pheromone_deposit_factor=1.0,\n",
    "           initial_pheromone=0.1, patience=7, verbose=True):\n",
    "    n_features = X_tr_fs.shape[1]\n",
    "    pheromones = np.full(n_features, initial_pheromone)\n",
    "    \n",
    "    global_best_mask = np.zeros(n_features, dtype=int)\n",
    "    global_best_fitness = -np.inf\n",
    "    \n",
    "    fitness_history = []\n",
    "    start_time = time.time()\n",
    "    stall_iter = 0\n",
    "    last_best_fitness_val = -np.inf\n",
    "\n",
    "    for iteration in range(max_iter):\n",
    "        current_iter_best_fitness = -np.inf\n",
    "        current_iter_best_mask = None\n",
    "\n",
    "        for ant in range(n_ants):\n",
    "            # Construct solution (select features based on pheromone levels)\n",
    "            # Higher pheromone -> higher probability of selection\n",
    "            pheromone_probs = pheromones / np.sum(pheromones)\n",
    "            # Ensure at least one feature is selected for meaningful evaluation\n",
    "            mask = (np.random.rand(n_features) < pheromone_probs).astype(int)\n",
    "            if np.sum(mask) == 0: # If no feature selected, randomly select one\n",
    "                mask[np.random.randint(0, n_features)] = 1\n",
    "            \n",
    "            num_selected = np.sum(mask)\n",
    "            acc, fpr = feature_fitness(mask, X_tr_fs, y_tr_fs, X_val_fs, y_val_fs)\n",
    "            current_fitness = _calculate_composite_fitness(acc, fpr, num_selected, n_features,\n",
    "                                                           ACC_WEIGHT, FPR_WEIGHT, FEAT_PENALTY_WEIGHT)\n",
    "\n",
    "            if current_fitness > current_iter_best_fitness:\n",
    "                current_iter_best_fitness = current_fitness\n",
    "                current_iter_best_mask = mask.copy()\n",
    "\n",
    "            if current_fitness > global_best_fitness:\n",
    "                global_best_fitness = current_fitness\n",
    "                global_best_mask = mask.copy()\n",
    "        \n",
    "        # Update pheromones\n",
    "        pheromones *= (1 - evaporation_rate)\n",
    "        if current_iter_best_mask is not None: # Deposit pheromone on the paths of good solutions\n",
    "             # Deposit more if fitness is positive, less or none if negative\n",
    "            deposit_amount = pheromone_deposit_factor * max(0, current_iter_best_fitness)\n",
    "            pheromones[current_iter_best_mask == 1] += deposit_amount\n",
    "        \n",
    "        pheromones = np.clip(pheromones, 1e-4, 1.0) # Keep pheromones within bounds\n",
    "\n",
    "        fitness_history.append(global_best_fitness)\n",
    "        _log_progress(\"ACO\", iteration, global_best_fitness, start_time, verbose)\n",
    "        \n",
    "        stall_iter, last_best_fitness_val = _check_early_stopping(global_best_fitness, last_best_fitness_val, stall_iter, patience)\n",
    "        if stall_iter >= patience:\n",
    "            if verbose: print(f\"[ACO] Early stopping at iteration {iteration+1}.\")\n",
    "            break\n",
    "            \n",
    "    return global_best_mask, fitness_history, time.time() - start_time\n",
    "\n",
    "\n",
    "def pso_fs(X_tr_fs, y_tr_fs, X_val_fs, y_val_fs,\n",
    "           n_particles=20, max_iter=30, w=0.7, c1=1.5, c2=1.5, # w: inertia, c1/c2: cognitive/social\n",
    "           patience=7, verbose=True):\n",
    "    n_features = X_tr_fs.shape[1]\n",
    "\n",
    "    # Initialize positions (continuous, to be binarized) and velocities\n",
    "    positions = np.random.rand(n_particles, n_features)\n",
    "    velocities = np.random.uniform(-0.1, 0.1, (n_particles, n_features))\n",
    "\n",
    "    # Personal bests\n",
    "    pbest_positions = positions.copy()\n",
    "    pbest_fitness = np.full(n_particles, -np.inf)\n",
    "\n",
    "    # Global best\n",
    "    gbest_position = np.zeros(n_features)\n",
    "    gbest_fitness = -np.inf\n",
    "    gbest_mask = np.zeros(n_features, dtype=int)\n",
    "\n",
    "    fitness_history = []\n",
    "    start_time = time.time()\n",
    "    stall_iter = 0\n",
    "    last_best_fitness_val = -np.inf\n",
    "\n",
    "    # Initial evaluation\n",
    "    for i in range(n_particles):\n",
    "        mask = _binarize_sigmoid(positions[i])\n",
    "        if np.sum(mask) == 0: mask[np.random.randint(0, n_features)] = 1\n",
    "        num_selected = np.sum(mask)\n",
    "        acc, fpr = feature_fitness(mask, X_tr_fs, y_tr_fs, X_val_fs, y_val_fs)\n",
    "        current_fitness = _calculate_composite_fitness(acc, fpr, num_selected, n_features,\n",
    "                                                       ACC_WEIGHT, FPR_WEIGHT, FEAT_PENALTY_WEIGHT)\n",
    "        pbest_fitness[i] = current_fitness\n",
    "        if current_fitness > gbest_fitness:\n",
    "            gbest_fitness = current_fitness\n",
    "            gbest_position = positions[i].copy()\n",
    "            gbest_mask = mask.copy()\n",
    "\n",
    "    fitness_history.append(gbest_fitness)\n",
    "\n",
    "    for iteration in range(max_iter):\n",
    "        # Update velocities and positions\n",
    "        for i in range(n_particles):\n",
    "            r1, r2 = np.random.rand(n_features), np.random.rand(n_features)\n",
    "            velocities[i] = (w * velocities[i] +\n",
    "                             c1 * r1 * (pbest_positions[i] - positions[i]) +\n",
    "                             c2 * r2 * (gbest_position - positions[i]))\n",
    "            positions[i] += velocities[i]\n",
    "            positions[i] = np.clip(positions[i], 0, 1) # Keep positions within [0,1] for sigmoid\n",
    "\n",
    "            # Evaluate new position\n",
    "            mask = _binarize_sigmoid(positions[i])\n",
    "            if np.sum(mask) == 0: mask[np.random.randint(0, n_features)] = 1 # Ensure at least one feature\n",
    "            num_selected = np.sum(mask)\n",
    "            acc, fpr = feature_fitness(mask, X_tr_fs, y_tr_fs, X_val_fs, y_val_fs)\n",
    "            current_fitness = _calculate_composite_fitness(acc, fpr, num_selected, n_features,\n",
    "                                                           ACC_WEIGHT, FPR_WEIGHT, FEAT_PENALTY_WEIGHT)\n",
    "\n",
    "            # Update personal best\n",
    "            if current_fitness > pbest_fitness[i]:\n",
    "                pbest_fitness[i] = current_fitness\n",
    "                pbest_positions[i] = positions[i].copy()\n",
    "\n",
    "            # Update global best\n",
    "            if current_fitness > gbest_fitness:\n",
    "                gbest_fitness = current_fitness\n",
    "                gbest_position = positions[i].copy()\n",
    "                gbest_mask = mask.copy()\n",
    "        \n",
    "        fitness_history.append(gbest_fitness)\n",
    "        _log_progress(\"PSO\", iteration, gbest_fitness, start_time, verbose)\n",
    "        \n",
    "        stall_iter, last_best_fitness_val = _check_early_stopping(gbest_fitness, last_best_fitness_val, stall_iter, patience)\n",
    "        if stall_iter >= patience:\n",
    "            if verbose: print(f\"[PSO] Early stopping at iteration {iteration+1}.\")\n",
    "            break\n",
    "            \n",
    "    return gbest_mask, fitness_history, time.time() - start_time\n",
    "\n",
    "\n",
    "def abc_fs(X_tr_fs, y_tr_fs, X_val_fs, y_val_fs,\n",
    "           n_bees=20, max_iter=30, limit=5, # limit: scout phase trigger\n",
    "           patience=7, verbose=True):\n",
    "    n_features = X_tr_fs.shape[1]\n",
    "    n_employed_bees = n_bees // 2\n",
    "    n_onlooker_bees = n_bees - n_employed_bees\n",
    "\n",
    "    # Food sources (solutions) - continuous, to be binarized\n",
    "    food_sources = np.random.rand(n_employed_bees, n_features)\n",
    "    food_fitness = np.full(n_employed_bees, -np.inf)\n",
    "    trials = np.zeros(n_employed_bees, dtype=int) # For scout phase\n",
    "\n",
    "    gbest_fitness = -np.inf\n",
    "    gbest_mask = np.zeros(n_features, dtype=int)\n",
    "    \n",
    "    fitness_history = []\n",
    "    start_time = time.time()\n",
    "    stall_iter = 0\n",
    "    last_best_fitness_val = -np.inf\n",
    "\n",
    "    # Initialize food sources and their fitness\n",
    "    for i in range(n_employed_bees):\n",
    "        mask = _binarize_sigmoid(food_sources[i])\n",
    "        if np.sum(mask) == 0: mask[np.random.randint(0, n_features)] = 1\n",
    "        num_selected = np.sum(mask)\n",
    "        acc, fpr = feature_fitness(mask, X_tr_fs, y_tr_fs, X_val_fs, y_val_fs)\n",
    "        food_fitness[i] = _calculate_composite_fitness(acc, fpr, num_selected, n_features,\n",
    "                                                       ACC_WEIGHT, FPR_WEIGHT, FEAT_PENALTY_WEIGHT)\n",
    "        if food_fitness[i] > gbest_fitness:\n",
    "            gbest_fitness = food_fitness[i]\n",
    "            gbest_mask = mask.copy()\n",
    "    fitness_history.append(gbest_fitness)\n",
    "\n",
    "    for iteration in range(max_iter):\n",
    "        # Employed bees phase\n",
    "        for i in range(n_employed_bees):\n",
    "            partner_idx = np.random.choice([j for j in range(n_employed_bees) if j != i])\n",
    "            phi = np.random.uniform(-1, 1, n_features)\n",
    "            candidate_solution = food_sources[i] + phi * (food_sources[i] - food_sources[partner_idx])\n",
    "            candidate_solution = np.clip(candidate_solution, 0, 1)\n",
    "\n",
    "            mask = _binarize_sigmoid(candidate_solution)\n",
    "            if np.sum(mask) == 0: mask[np.random.randint(0, n_features)] = 1\n",
    "            num_selected = np.sum(mask)\n",
    "            acc, fpr = feature_fitness(mask, X_tr_fs, y_tr_fs, X_val_fs, y_val_fs)\n",
    "            candidate_fitness = _calculate_composite_fitness(acc, fpr, num_selected, n_features,\n",
    "                                                             ACC_WEIGHT, FPR_WEIGHT, FEAT_PENALTY_WEIGHT)\n",
    "\n",
    "            if candidate_fitness > food_fitness[i]:\n",
    "                food_sources[i] = candidate_solution\n",
    "                food_fitness[i] = candidate_fitness\n",
    "                trials[i] = 0\n",
    "            else:\n",
    "                trials[i] += 1\n",
    "        \n",
    "        # Onlooker bees phase\n",
    "        # Calculate selection probabilities based on fitness (roulette wheel)\n",
    "        fitness_values = np.maximum(food_fitness, 0) # Avoid negative probabilities if all fitnesses are negative\n",
    "        if np.sum(fitness_values) > 0:\n",
    "            probs = fitness_values / np.sum(fitness_values)\n",
    "        else: # If all fitnesses are zero or negative, equal probability\n",
    "            probs = np.ones(n_employed_bees) / n_employed_bees\n",
    "\n",
    "        for _ in range(n_onlooker_bees):\n",
    "            chosen_source_idx = np.random.choice(n_employed_bees, p=probs)\n",
    "            \n",
    "            partner_idx = np.random.choice([j for j in range(n_employed_bees) if j != chosen_source_idx])\n",
    "            phi = np.random.uniform(-1, 1, n_features)\n",
    "            candidate_solution = food_sources[chosen_source_idx] + phi * (food_sources[chosen_source_idx] - food_sources[partner_idx])\n",
    "            candidate_solution = np.clip(candidate_solution, 0, 1)\n",
    "\n",
    "            mask = _binarize_sigmoid(candidate_solution)\n",
    "            if np.sum(mask) == 0: mask[np.random.randint(0, n_features)] = 1\n",
    "            num_selected = np.sum(mask)\n",
    "            acc, fpr = feature_fitness(mask, X_tr_fs, y_tr_fs, X_val_fs, y_val_fs)\n",
    "            candidate_fitness = _calculate_composite_fitness(acc, fpr, num_selected, n_features,\n",
    "                                                             ACC_WEIGHT, FPR_WEIGHT, FEAT_PENALTY_WEIGHT)\n",
    "\n",
    "            if candidate_fitness > food_fitness[chosen_source_idx]:\n",
    "                food_sources[chosen_source_idx] = candidate_solution\n",
    "                food_fitness[chosen_source_idx] = candidate_fitness\n",
    "                trials[chosen_source_idx] = 0\n",
    "            else:\n",
    "                trials[chosen_source_idx] += 1\n",
    "\n",
    "        # Scout bees phase\n",
    "        for i in range(n_employed_bees):\n",
    "            if trials[i] >= limit:\n",
    "                food_sources[i] = np.random.rand(n_features) # New random food source\n",
    "                mask = _binarize_sigmoid(food_sources[i])\n",
    "                if np.sum(mask) == 0: mask[np.random.randint(0, n_features)] = 1\n",
    "                num_selected = np.sum(mask)\n",
    "                acc, fpr = feature_fitness(mask, X_tr_fs, y_tr_fs, X_val_fs, y_val_fs)\n",
    "                food_fitness[i] = _calculate_composite_fitness(acc, fpr, num_selected, n_features,\n",
    "                                                               ACC_WEIGHT, FPR_WEIGHT, FEAT_PENALTY_WEIGHT)\n",
    "                trials[i] = 0\n",
    "        \n",
    "        # Update global best\n",
    "        current_best_idx = np.argmax(food_fitness)\n",
    "        if food_fitness[current_best_idx] > gbest_fitness:\n",
    "            gbest_fitness = food_fitness[current_best_idx]\n",
    "            gbest_mask = _binarize_sigmoid(food_sources[current_best_idx])\n",
    "\n",
    "        fitness_history.append(gbest_fitness)\n",
    "        _log_progress(\"ABC\", iteration, gbest_fitness, start_time, verbose)\n",
    "        \n",
    "        stall_iter, last_best_fitness_val = _check_early_stopping(gbest_fitness, last_best_fitness_val, stall_iter, patience)\n",
    "        if stall_iter >= patience:\n",
    "            if verbose: print(f\"[ABC] Early stopping at iteration {iteration+1}.\")\n",
    "            break\n",
    "            \n",
    "    return gbest_mask, fitness_history, time.time() - start_time\n",
    "\n",
    "\n",
    "def mwpa_fs(X_tr_fs, y_tr_fs, X_val_fs, y_val_fs,\n",
    "            n_wolves=20, max_iter=30, beta_mwpa=1.5, # beta_mwpa for exploration/exploitation balance\n",
    "            patience=7, verbose=True):\n",
    "    n_features = X_tr_fs.shape[1]\n",
    "\n",
    "    # Wolf positions (continuous, to be binarized)\n",
    "    wolf_positions = np.random.rand(n_wolves, n_features)\n",
    "    \n",
    "    # Alpha wolf (best solution found so far)\n",
    "    alpha_pos = np.zeros(n_features)\n",
    "    alpha_fitness = -np.inf\n",
    "    alpha_mask = np.zeros(n_features, dtype=int)\n",
    "\n",
    "    fitness_history = []\n",
    "    start_time = time.time()\n",
    "    stall_iter = 0\n",
    "    last_best_fitness_val = -np.inf\n",
    "\n",
    "    # Initial evaluation to find the alpha wolf\n",
    "    for i in range(n_wolves):\n",
    "        mask = _binarize_sigmoid(wolf_positions[i])\n",
    "        if np.sum(mask) == 0: mask[np.random.randint(0, n_features)] = 1\n",
    "        num_selected = np.sum(mask)\n",
    "        acc, fpr = feature_fitness(mask, X_tr_fs, y_tr_fs, X_val_fs, y_val_fs)\n",
    "        current_fitness = _calculate_composite_fitness(acc, fpr, num_selected, n_features,\n",
    "                                                       ACC_WEIGHT, FPR_WEIGHT, FEAT_PENALTY_WEIGHT)\n",
    "        if current_fitness > alpha_fitness:\n",
    "            alpha_fitness = current_fitness\n",
    "            alpha_pos = wolf_positions[i].copy()\n",
    "            alpha_mask = mask.copy()\n",
    "    fitness_history.append(alpha_fitness)\n",
    "\n",
    "    for iteration in range(max_iter):\n",
    "        # Parameter 'a' decreases from 2 to 0 (controls exploration/exploitation)\n",
    "        # Original GWO has alpha, beta, delta wolves. MWPA simplifies to just alpha.\n",
    "        a_param = 2 - 2 * (iteration / max_iter) \n",
    "\n",
    "        for i in range(n_wolves):\n",
    "            # Update position of current wolf based on alpha wolf\n",
    "            r1, r2 = np.random.rand(n_features), np.random.rand(n_features) # Random vectors\n",
    "            \n",
    "            A_vector = 2 * a_param * r1 - a_param # Coefficient vector A\n",
    "            # C_vector = 2 * r2 # Coefficient vector C (Original GWO uses this, MWPA might simplify)\n",
    "            \n",
    "            # Distance D to the alpha wolf (prey)\n",
    "            D_alpha = np.abs(2 * r2 * alpha_pos - wolf_positions[i]) # Simplified D from MWPA-like approaches\n",
    "            \n",
    "            # Update wolf position\n",
    "            wolf_positions[i] = alpha_pos - A_vector * (D_alpha**beta_mwpa) # MWPA-like update\n",
    "            wolf_positions[i] = np.clip(wolf_positions[i], 0, 1)\n",
    "\n",
    "            # Evaluate new position\n",
    "            mask = _binarize_sigmoid(wolf_positions[i])\n",
    "            if np.sum(mask) == 0: mask[np.random.randint(0, n_features)] = 1\n",
    "            num_selected = np.sum(mask)\n",
    "            acc, fpr = feature_fitness(mask, X_tr_fs, y_tr_fs, X_val_fs, y_val_fs)\n",
    "            current_fitness = _calculate_composite_fitness(acc, fpr, num_selected, n_features,\n",
    "                                                           ACC_WEIGHT, FPR_WEIGHT, FEAT_PENALTY_WEIGHT)\n",
    "            \n",
    "            # Update alpha wolf if current wolf is better\n",
    "            if current_fitness > alpha_fitness:\n",
    "                alpha_fitness = current_fitness\n",
    "                alpha_pos = wolf_positions[i].copy()\n",
    "                alpha_mask = mask.copy()\n",
    "        \n",
    "        fitness_history.append(alpha_fitness)\n",
    "        _log_progress(\"MWPA\", iteration, alpha_fitness, start_time, verbose)\n",
    "        \n",
    "        stall_iter, last_best_fitness_val = _check_early_stopping(alpha_fitness, last_best_fitness_val, stall_iter, patience)\n",
    "        if stall_iter >= patience:\n",
    "            if verbose: print(f\"[MWPA] Early stopping at iteration {iteration+1}.\")\n",
    "            break\n",
    "            \n",
    "    return alpha_mask, fitness_history, time.time() - start_time\n",
    "\n",
    "print(\"Individual swarm intelligence algorithms (ACO, PSO, ABC, MWPA) for feature selection defined.\")\n",
    "print(f\"Fitness composition: ACC_WEIGHT={ACC_WEIGHT}, FPR_WEIGHT={FPR_WEIGHT}, FEAT_PENALTY_WEIGHT={FEAT_PENALTY_WEIGHT}\")\n",
    "```\n",
    "\n",
    "**Rationale for Step 4 Modifications:** 1. **Fitness Calculation:** \\*\n",
    "All individual algorithms (ACO, PSO, ABC, MWPA) now call the\n",
    "`feature_fitness` function (from Step 3), which returns\n",
    "`(accuracy, fpr)`. \\* They then use a new helper function\n",
    "`_calculate_composite_fitness` to combine these metrics with a penalty\n",
    "for the number of selected features. This composite fitness is\n",
    "`(acc_weight * accuracy) - (fpr_weight * fpr) - (feat_penalty_weight * feature_ratio)`.\n",
    "\\* Global constants `ACC_WEIGHT`, `FPR_WEIGHT`, and\n",
    "`FEAT_PENALTY_WEIGHT` are defined to control the trade-offs.\n",
    "`FPR_WEIGHT` is set to `0.7` to give a significant penalty to false\n",
    "positives, aiming for the user’s goal. `FEAT_PENALTY_WEIGHT` is kept\n",
    "relatively small. 2. **Binarization:** \\* The `_binarize` function was\n",
    "renamed to `_binarize_sigmoid` for clarity. It now uses a slightly\n",
    "scaled sigmoid (`-10 * (continuous_values - threshold)`) to make the\n",
    "transition sharper around the threshold, which can sometimes be\n",
    "beneficial. 3. **Ensuring Feature Selection:** \\* A check\n",
    "`if np.sum(mask) == 0:` is added after binarization in each algorithm.\n",
    "If no features are selected (which would lead to errors or meaningless\n",
    "evaluations), one feature is randomly selected. This ensures the fitness\n",
    "function always receives a valid subset. 4. **Parameter Naming and\n",
    "Defaults:** \\* Algorithm-specific parameters (e.g., `n_ants`,\n",
    "`evaporation_rate` for ACO; `n_particles`, `w`, `c1`, `c2` for PSO) are\n",
    "clearly defined in function signatures with reasonable default values.\n",
    "\\* `patience` for early stopping and `verbose` flags are standardized.\n",
    "\\* Default iterations (`max_iter=30`) and agent counts (`n_ants=20`,\n",
    "etc.) are set to moderate values for quicker runs during\n",
    "development/demonstration. For thorough experiments, these would\n",
    "typically be higher. 5. **Helper Functions:** \\* `_log_progress` and\n",
    "`_check_early_stopping` are refined for clarity. 6. **ACO Pheromone\n",
    "Update:** \\* Pheromone deposit is now proportional to\n",
    "`max(0, current_iter_best_fitness)`. This means only solutions with\n",
    "positive (good) composite fitness contribute significantly to pheromone\n",
    "trails. \\* Pheromones are clipped to prevent extreme values. 7. **ABC\n",
    "Onlooker Probability:** \\* Handled the case where all food source\n",
    "fitnesses might be negative or zero by assigning equal probability to\n",
    "prevent errors with `np.random.choice`. 8. **MWPA Update:** \\* The MWPA\n",
    "update rule was kept similar to the original structure, focusing on the\n",
    "alpha wolf. The `beta_mwpa` parameter is exposed. The coefficient\n",
    "`A_vector` and distance `D_alpha` are calculated based on common\n",
    "GWO/MWPA formulations.\n",
    "\n",
    "These changes ensure that all individual algorithms optimize for the new\n",
    "composite fitness metric, which explicitly penalizes false positives and\n",
    "the number of features, while rewarding accuracy.\n",
    "\n",
    "``` python\n",
    "# 5. Hybrid Swarm (where you should innovate)\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "# Assuming feature_fitness, _binarize_sigmoid, _calculate_composite_fitness,\n",
    "# _log_progress, _check_early_stopping are defined in the global scope (e.g. from step 3 & 4)\n",
    "\n",
    "# Use the same global weights for consistency with individual algorithms\n",
    "# ACC_WEIGHT, FPR_WEIGHT, FEAT_PENALTY_WEIGHT should be defined (e.g., from step 4)\n",
    "# If not, define them here or pass them as parameters. For this example, assume they are global.\n",
    "# ACC_WEIGHT = 1.0\n",
    "# FPR_WEIGHT = 0.7\n",
    "# FEAT_PENALTY_WEIGHT = 0.02\n",
    "\n",
    "class HybridSwarmFeatureSelector:\n",
    "    def __init__(self, n_features, n_agents=30, max_iter=50,\n",
    "                 # Operator-specific parameters\n",
    "                 pso_w_range=(0.4, 0.9), pso_c1=1.5, pso_c2=1.5, # PSO: inertia weight range, cognitive/social factors\n",
    "                 aco_evap_rate=0.1, aco_deposit_factor=1.0, aco_initial_pheromone=0.1, # ACO\n",
    "                 abc_limit=7, # ABC: scout limit\n",
    "                 mwpa_beta_range=(0.5, 1.5), # MWPA: exploration factor range\n",
    "                 # Hybrid control\n",
    "                 operator_probs_initial=None, # Initial probabilities for choosing operators\n",
    "                 adaptive_operator_selection_lr=0.1, # Learning rate for updating operator probabilities\n",
    "                 elite_pool_size=5, # Number of elite solutions to maintain\n",
    "                 stagnation_reset_patience=10, # Iterations of no improvement to trigger partial reset\n",
    "                 verbose=True):\n",
    "\n",
    "        self.n_features = int(n_features)\n",
    "        self.n_agents = int(n_agents)\n",
    "        self.max_iter = int(max_iter)\n",
    "        \n",
    "        # PSO params\n",
    "        self.pso_w_min, self.pso_w_max = pso_w_range\n",
    "        self.pso_c1 = float(pso_c1)\n",
    "        self.pso_c2 = float(pso_c2)\n",
    "        \n",
    "        # ACO params\n",
    "        self.aco_evap_rate = float(aco_evap_rate)\n",
    "        self.aco_deposit_factor = float(aco_deposit_factor)\n",
    "        self.aco_initial_pheromone = float(aco_initial_pheromone)\n",
    "        \n",
    "        # ABC params\n",
    "        self.abc_limit = int(abc_limit)\n",
    "        \n",
    "        # MWPA params\n",
    "        self.mwpa_beta_min, self.mwpa_beta_max = mwpa_beta_range\n",
    "\n",
    "        # Hybrid control\n",
    "        self.operators = ['PSO', 'ACO', 'ABC', 'MWPA_Enhanced']\n",
    "        if operator_probs_initial is None or len(operator_probs_initial) != len(self.operators):\n",
    "            self.operator_probs = np.full(len(self.operators), 1.0 / len(self.operators))\n",
    "        else:\n",
    "            self.operator_probs = np.array(operator_probs_initial, dtype=float)\n",
    "        self.adaptive_operator_lr = float(adaptive_operator_selection_lr)\n",
    "        self.operator_rewards = np.zeros(len(self.operators)) # Cumulative rewards for operators\n",
    "        self.operator_counts = np.zeros(len(self.operators)) + 1e-6 # Usage counts for operators\n",
    "\n",
    "        self.elite_pool_size = int(elite_pool_size)\n",
    "        self.elite_solutions = [] # Stores (fitness, mask) tuples\n",
    "\n",
    "        self.stagnation_reset_patience = stagnation_reset_patience\n",
    "        self.verbose = verbose\n",
    "\n",
    "        # Agent states (shared or adapted by operators)\n",
    "        self.agent_positions = np.random.rand(self.n_agents, self.n_features) # Continuous positions\n",
    "        self.agent_velocities = np.random.uniform(-0.1, 0.1, (self.n_agents, self.n_features)) # For PSO\n",
    "        self.agent_fitness = np.full(self.n_agents, -np.inf)\n",
    "        self.agent_masks = np.array([_binarize_sigmoid(pos) for pos in self.agent_positions])\n",
    "\n",
    "        self.pbest_positions = self.agent_positions.copy()\n",
    "        self.pbest_fitness = np.full(self.n_agents, -np.inf)\n",
    "\n",
    "        self.gbest_fitness = -np.inf\n",
    "        self.gbest_mask = np.zeros(self.n_features, dtype=int)\n",
    "        self.gbest_position = np.zeros(self.n_features) # Continuous position for gbest\n",
    "\n",
    "        self.pheromones = np.full(self.n_features, self.aco_initial_pheromone) # For ACO\n",
    "        self.abc_trials = np.zeros(self.n_agents, dtype=int) # For ABC scout phase\n",
    "\n",
    "        # Tracking\n",
    "        self.fitness_history = []\n",
    "        self.time_taken = 0.0\n",
    "        \n",
    "        # Data (will be set by run method)\n",
    "        self.X_tr, self.y_tr, self.X_val, self.y_val = None, None, None, None\n",
    "\n",
    "\n",
    "    def _evaluate_agent(self, agent_idx):\n",
    "        mask = _binarize_sigmoid(self.agent_positions[agent_idx])\n",
    "        if np.sum(mask) == 0: # Ensure at least one feature\n",
    "            mask[np.random.randint(0, self.n_features)] = 1\n",
    "        \n",
    "        self.agent_masks[agent_idx] = mask # Store the binarized mask\n",
    "        num_selected = np.sum(mask)\n",
    "        \n",
    "        acc, fpr = feature_fitness(mask, self.X_tr, self.y_tr, self.X_val, self.y_val)\n",
    "        current_fitness = _calculate_composite_fitness(acc, fpr, num_selected, self.n_features,\n",
    "                                                       ACC_WEIGHT, FPR_WEIGHT, FEAT_PENALTY_WEIGHT)\n",
    "        return current_fitness, mask\n",
    "\n",
    "    def _initialize_population(self):\n",
    "        self.agent_positions = np.random.rand(self.n_agents, self.n_features)\n",
    "        self.agent_velocities = np.random.uniform(-0.1, 0.1, (self.n_agents, self.n_features))\n",
    "        self.pheromones.fill(self.aco_initial_pheromone)\n",
    "        self.abc_trials.fill(0)\n",
    "        self.operator_rewards.fill(0)\n",
    "        self.operator_counts.fill(1e-6)\n",
    "\n",
    "        for i in range(self.n_agents):\n",
    "            fitness, mask = self._evaluate_agent(i)\n",
    "            self.agent_fitness[i] = fitness\n",
    "            self.pbest_positions[i] = self.agent_positions[i].copy()\n",
    "            self.pbest_fitness[i] = fitness\n",
    "            \n",
    "            if fitness > self.gbest_fitness:\n",
    "                self.gbest_fitness = fitness\n",
    "                self.gbest_mask = mask.copy()\n",
    "                self.gbest_position = self.agent_positions[i].copy()\n",
    "        \n",
    "        self.fitness_history.append(self.gbest_fitness)\n",
    "        self._update_elite_pool()\n",
    "\n",
    "    def _update_elite_pool(self):\n",
    "        # Add current gbest to pool if it's good enough\n",
    "        if self.gbest_fitness > -np.inf:\n",
    "            self.elite_solutions.append((self.gbest_fitness, self.gbest_mask.copy()))\n",
    "        # Sort by fitness (descending) and keep top K\n",
    "        self.elite_solutions = sorted(self.elite_solutions, key=lambda x: x[0], reverse=True)\n",
    "        self.elite_solutions = self.elite_solutions[:self.elite_pool_size]\n",
    "\n",
    "    def _select_operator_adaptively(self):\n",
    "        # UCB1-like mechanism or simple reward-based probability adjustment\n",
    "        # For simplicity, use current probabilities, update them later based on reward\n",
    "        if np.sum(self.operator_probs) == 0 : # safety check\n",
    "            self.operator_probs = np.full(len(self.operators), 1.0 / len(self.operators))\n",
    "\n",
    "        op_idx = np.random.choice(len(self.operators), p=self.operator_probs)\n",
    "        return op_idx, self.operators[op_idx]\n",
    "\n",
    "    def _update_operator_probabilities(self, op_idx, reward):\n",
    "        # Update rewards and counts\n",
    "        self.operator_rewards[op_idx] += reward\n",
    "        self.operator_counts[op_idx] += 1\n",
    "        \n",
    "        # Update probabilities using a simple normalization of average rewards\n",
    "        # Add a small exploration factor to prevent probabilities from going to zero\n",
    "        avg_rewards = self.operator_rewards / self.operator_counts\n",
    "        # Normalize: shift to be non-negative, then softmax-like scaling\n",
    "        min_reward = np.min(avg_rewards)\n",
    "        scaled_rewards = np.exp(self.adaptive_operator_lr * (avg_rewards - min_reward)) \n",
    "        self.operator_probs = scaled_rewards / np.sum(scaled_rewards)\n",
    "\n",
    "\n",
    "    def _apply_operator(self, agent_idx, operator_name, iteration):\n",
    "        pos = self.agent_positions[agent_idx].copy()\n",
    "        original_fitness = self.agent_fitness[agent_idx]\n",
    "\n",
    "        # Dynamic parameters based on iteration\n",
    "        # PSO: Linearly decreasing inertia weight\n",
    "        pso_w = self.pso_w_max - (self.pso_w_max - self.pso_w_min) * (iteration / self.max_iter)\n",
    "        # MWPA: Linearly changing beta for exploration/exploitation balance\n",
    "        mwpa_beta = self.mwpa_beta_min + (self.mwpa_beta_max - self.mwpa_beta_min) * (iteration / self.max_iter)\n",
    "        mwpa_a = 2.0 - 2.0 * (iteration / self.max_iter) # GWO 'a' parameter\n",
    "\n",
    "        if operator_name == 'PSO':\n",
    "            r1, r2 = np.random.rand(self.n_features), np.random.rand(self.n_features)\n",
    "            self.agent_velocities[agent_idx] = (pso_w * self.agent_velocities[agent_idx] +\n",
    "                                                self.pso_c1 * r1 * (self.pbest_positions[agent_idx] - pos) +\n",
    "                                                self.pso_c2 * r2 * (self.gbest_position - pos))\n",
    "            self.agent_positions[agent_idx] = pos + self.agent_velocities[agent_idx]\n",
    "\n",
    "        elif operator_name == 'ACO':\n",
    "            # ACO uses pheromones to guide search, can be a perturbation or construction\n",
    "            # Here, a perturbation approach: move towards gbest influenced by pheromones\n",
    "            pheromone_influence = self.pheromones / np.sum(self.pheromones)\n",
    "            random_perturb = (np.random.rand(self.n_features) - 0.5) * 0.1 # Small random step\n",
    "            # Move towards a weighted combination of gbest and random, guided by pheromones\n",
    "            self.agent_positions[agent_idx] = pos + pheromone_influence * (self.gbest_position - pos) * np.random.rand() + random_perturb\n",
    "\n",
    "\n",
    "        elif operator_name == 'ABC':\n",
    "            partner_idx = np.random.choice([j for j in range(self.n_agents) if j != agent_idx])\n",
    "            phi = np.random.uniform(-1, 1, self.n_features)\n",
    "            self.agent_positions[agent_idx] = pos + phi * (pos - self.agent_positions[partner_idx])\n",
    "            # ABC trial update will be handled after evaluation\n",
    "\n",
    "        elif operator_name == 'MWPA_Enhanced':\n",
    "            # Use one of the elite solutions as the \"alpha\" for diversity, or gbest\n",
    "            target_pos = self.gbest_position\n",
    "            if self.elite_solutions and np.random.rand() < 0.3: # 30% chance to use an elite\n",
    "                elite_fitness, elite_mask = self.elite_solutions[np.random.randint(len(self.elite_solutions))]\n",
    "                # Need continuous position for elite, if not stored, derive from mask or use gbest\n",
    "                # For simplicity, if elite_mask is used, it implies a binarized target.\n",
    "                # Let's assume MWPA works with continuous target_pos (gbest_position or elite's continuous pos if stored)\n",
    "                pass # Using self.gbest_position as target_pos\n",
    "\n",
    "            r1, r2 = np.random.rand(self.n_features), np.random.rand(self.n_features)\n",
    "            A_vec = 2 * mwpa_a * r1 - mwpa_a\n",
    "            D_vec = np.abs(2 * r2 * target_pos - pos)\n",
    "            self.agent_positions[agent_idx] = target_pos - A_vec * (D_vec**mwpa_beta)\n",
    "\n",
    "        # Clip to bounds [0,1]\n",
    "        self.agent_positions[agent_idx] = np.clip(self.agent_positions[agent_idx], 0, 1)\n",
    "        \n",
    "        # Evaluate new position\n",
    "        new_fitness, new_mask = self._evaluate_agent(agent_idx)\n",
    "        \n",
    "        # Update personal best\n",
    "        if new_fitness > self.pbest_fitness[agent_idx]:\n",
    "            self.pbest_fitness[agent_idx] = new_fitness\n",
    "            self.pbest_positions[agent_idx] = self.agent_positions[agent_idx].copy()\n",
    "        \n",
    "        # Update global best\n",
    "        if new_fitness > self.gbest_fitness:\n",
    "            self.gbest_fitness = new_fitness\n",
    "            self.gbest_mask = new_mask.copy()\n",
    "            self.gbest_position = self.agent_positions[agent_idx].copy()\n",
    "            self._update_elite_pool() # Update elite pool if gbest improved\n",
    "\n",
    "        # For ABC: update trial counter\n",
    "        if operator_name == 'ABC': # Could be generalized\n",
    "            if new_fitness > self.agent_fitness[agent_idx]:\n",
    "                self.abc_trials[agent_idx] = 0\n",
    "            else:\n",
    "                self.abc_trials[agent_idx] += 1\n",
    "        \n",
    "        self.agent_fitness[agent_idx] = new_fitness # Update agent's current fitness\n",
    "\n",
    "        # Calculate reward for the operator\n",
    "        reward = new_fitness - original_fitness if new_fitness > original_fitness else 0\n",
    "        return reward\n",
    "\n",
    "\n",
    "    def _aco_pheromone_update(self):\n",
    "        self.pheromones *= (1 - self.aco_evap_rate)\n",
    "        # Deposit on gbest_mask and elite solutions\n",
    "        solutions_to_deposit = [(self.gbest_fitness, self.gbest_mask)] + self.elite_solutions\n",
    "        \n",
    "        for fit, mask_to_deposit in solutions_to_deposit:\n",
    "            if np.sum(mask_to_deposit) > 0: # Ensure mask is valid\n",
    "                deposit_value = self.aco_deposit_factor * max(0, fit) # Deposit based on positive fitness\n",
    "                self.pheromones[mask_to_deposit == 1] += deposit_value\n",
    "        \n",
    "        self.pheromones = np.clip(self.pheromones, 1e-4, 1.0) # Min/max pheromone\n",
    "\n",
    "    def _abc_scout_phase(self):\n",
    "        for i in range(self.n_agents): # Check all agents for potential scout phase (if ABC was applied)\n",
    "            if self.abc_trials[i] >= self.abc_limit:\n",
    "                self.agent_positions[i] = np.random.rand(self.n_features) # Reset position\n",
    "                self.abc_trials[i] = 0\n",
    "                # Re-evaluate this new scout position\n",
    "                fitness, mask = self._evaluate_agent(i)\n",
    "                self.agent_fitness[i] = fitness\n",
    "                if fitness > self.pbest_fitness[i]:\n",
    "                    self.pbest_fitness[i] = fitness\n",
    "                    self.pbest_positions[i] = self.agent_positions[i].copy()\n",
    "                if fitness > self.gbest_fitness:\n",
    "                    self.gbest_fitness = fitness\n",
    "                    self.gbest_mask = mask.copy()\n",
    "                    self.gbest_position = self.agent_positions[i].copy()\n",
    "\n",
    "\n",
    "    def _handle_stagnation_and_diversify(self, iteration, last_improvement_iter):\n",
    "        if iteration - last_improvement_iter > self.stagnation_reset_patience:\n",
    "            if self.verbose: print(f\"[Hybrid] Stagnation detected at iter {iteration+1}. Performing partial reset.\")\n",
    "            num_reset_agents = self.n_agents // 3 # Reset worst 1/3 agents\n",
    "            worst_agent_indices = np.argsort(self.agent_fitness)[:num_reset_agents]\n",
    "            \n",
    "            for i in worst_agent_indices:\n",
    "                # Reset strategy: new random position or mutated elite\n",
    "                if self.elite_solutions and np.random.rand() < 0.5:\n",
    "                    _, elite_mask = self.elite_solutions[np.random.randint(len(self.elite_solutions))]\n",
    "                    # Create a continuous position that would likely produce this mask, then perturb\n",
    "                    temp_pos = np.where(elite_mask == 1, \n",
    "                                        np.random.uniform(0.6, 1.0, self.n_features), \n",
    "                                        np.random.uniform(0.0, 0.4, self.n_features))\n",
    "                    # Add noise for diversity\n",
    "                    self.agent_positions[i] = np.clip(temp_pos + np.random.normal(0, 0.1, self.n_features), 0, 1)\n",
    "                else:\n",
    "                    self.agent_positions[i] = np.random.rand(self.n_features)\n",
    "                \n",
    "                # Re-evaluate\n",
    "                fitness, mask = self._evaluate_agent(i)\n",
    "                self.agent_fitness[i] = fitness\n",
    "                self.pbest_positions[i] = self.agent_positions[i].copy()\n",
    "                self.pbest_fitness[i] = fitness\n",
    "                # Update gbest if this reset agent found something better (unlikely but possible)\n",
    "                if fitness > self.gbest_fitness:\n",
    "                    self.gbest_fitness = fitness\n",
    "                    self.gbest_mask = mask.copy()\n",
    "                    self.gbest_position = self.agent_positions[i].copy()\n",
    "\n",
    "            return iteration # Reset last_improvement_iter\n",
    "\n",
    "        return last_improvement_iter\n",
    "\n",
    "\n",
    "    def run(self, X_tr_fs, y_tr_fs, X_val_fs, y_val_fs, patience=10):\n",
    "        self.X_tr, self.y_tr, self.X_val, self.y_val = X_tr_fs, y_tr_fs, X_val_fs, y_val_fs\n",
    "        \n",
    "        if self.n_features == 0: # Handle case where input data might be empty\n",
    "             if self.verbose: print(\"[Hybrid] No features to select from.\")\n",
    "             return np.array([]), [], 0.0\n",
    "\n",
    "        start_time = time.time()\n",
    "        self._initialize_population()\n",
    "\n",
    "        stall_iter = 0\n",
    "        last_best_fitness_val = self.gbest_fitness\n",
    "        last_improvement_iter = 0\n",
    "\n",
    "        for iteration in range(self.max_iter):\n",
    "            current_gbest_before_iter = self.gbest_fitness\n",
    "\n",
    "            for i in range(self.n_agents):\n",
    "                op_idx, op_name = self._select_operator_adaptively()\n",
    "                reward = self._apply_operator(i, op_name, iteration)\n",
    "                self._update_operator_probabilities(op_idx, reward) # Update probabilities based on reward\n",
    "\n",
    "            self._aco_pheromone_update() # Global pheromone update after all agents move\n",
    "            self._abc_scout_phase()      # Check for scouts after all agents move\n",
    "\n",
    "            if self.gbest_fitness > current_gbest_before_iter:\n",
    "                last_improvement_iter = iteration\n",
    "            \n",
    "            last_improvement_iter = self._handle_stagnation_and_diversify(iteration, last_improvement_iter)\n",
    "\n",
    "            self.fitness_history.append(self.gbest_fitness)\n",
    "            if self.verbose:\n",
    "                 _log_progress(\"Hybrid\", iteration, self.gbest_fitness, start_time, 1 if self.verbose else 0)\n",
    "                 if iteration % 10 == 0: # Periodically print operator probabilities\n",
    "                     op_probs_str = \", \".join([f\"{op}: {prob:.2f}\" for op, prob in zip(self.operators, self.operator_probs)])\n",
    "                     print(f\"[Hybrid] Iter {iteration+1} Operator Probs: {op_probs_str}\")\n",
    "\n",
    "\n",
    "            stall_iter, last_best_fitness_val = _check_early_stopping(self.gbest_fitness, last_best_fitness_val, stall_iter, patience)\n",
    "            if stall_iter >= patience:\n",
    "                if self.verbose: print(f\"[Hybrid] Early stopping at iteration {iteration+1}.\")\n",
    "                break\n",
    "        \n",
    "        self.time_taken = time.time() - start_time\n",
    "        \n",
    "        # Final selection from elite pool or gbest\n",
    "        final_best_mask = self.gbest_mask\n",
    "        final_best_fitness = self.gbest_fitness\n",
    "        if self.elite_solutions:\n",
    "            best_elite_fitness, best_elite_mask = self.elite_solutions[0] # Elites are sorted\n",
    "            if best_elite_fitness > final_best_fitness : # Should not happen if gbest is always added to elites\n",
    "                final_best_mask = best_elite_mask\n",
    "                if self.verbose: print(f\"[Hybrid] Selected best mask from elite pool with fitness {best_elite_fitness:.4f}.\")\n",
    "            elif self.verbose:\n",
    "                 print(f\"[Hybrid] Final gbest fitness: {self.gbest_fitness:.4f}. Elite pool best: {best_elite_fitness:.4f}\")\n",
    "\n",
    "\n",
    "        if self.verbose:\n",
    "            print(f\"[Hybrid] Optimization finished. Best fitness: {self.gbest_fitness:.5f}\")\n",
    "            print(f\"[Hybrid] Selected {np.sum(final_best_mask)} features.\")\n",
    "            \n",
    "        return final_best_mask, self.fitness_history, self.time_taken\n",
    "\n",
    "print(\"Hybrid Swarm Feature Selector defined.\")\n",
    "```\n",
    "\n",
    "**Rationale for Step 5 Modifications (Hybrid Swarm):**\n",
    "\n",
    "1.  **Fitness Calculation:**\n",
    "    -   The hybrid algorithm now uses the same fitness evaluation\n",
    "        mechanism as the individual algorithms: `_evaluate_agent` calls\n",
    "        the global `feature_fitness` (returns `acc, fpr`) and then\n",
    "        `_calculate_composite_fitness` (uses global `ACC_WEIGHT`,\n",
    "        `FPR_WEIGHT`, `FEAT_PENALTY_WEIGHT`). This ensures consistency\n",
    "        in how solutions are evaluated across all methods.\n",
    "2.  **Adaptive Operator Selection (AOS):**\n",
    "    -   The mechanism for selecting operators\n",
    "        (`_select_operator_adaptively`) and updating their probabilities\n",
    "        (`_update_operator_probabilities`) is refined.\n",
    "    -   It now uses a reward-based system. The reward is the improvement\n",
    "        in fitness an operator achieves for an agent.\n",
    "    -   Operator probabilities are updated using a softmax-like scaling\n",
    "        of their average rewards, with a learning rate\n",
    "        (`adaptive_operator_lr`). This allows the hybrid to learn which\n",
    "        operators are more effective over time.\n",
    "3.  **Operator Implementations (`_apply_operator`):**\n",
    "    -   Each operator (PSO, ACO, ABC, MWPA_Enhanced) is implemented as a\n",
    "        distinct behavior that modifies an agent’s position.\n",
    "    -   **PSO:** Standard velocity and position update. Inertia weight\n",
    "        `pso_w` decreases linearly.\n",
    "    -   **ACO:** Pheromones guide the perturbation of an agent’s\n",
    "        position, typically towards `gbest_position`. This is a\n",
    "        simplified ACO move within a hybrid context.\n",
    "    -   **ABC:** Standard ABC employed bee-like update using a partner\n",
    "        solution.\n",
    "    -   **MWPA_Enhanced:** A GWO-like update. It can target\n",
    "        `gbest_position` or, with some probability, a solution from the\n",
    "        elite pool to enhance diversity. The `a` parameter\n",
    "        (exploration/exploitation) and `beta` (movement style) are\n",
    "        dynamic.\n",
    "4.  **Elite Pool:**\n",
    "    -   An `elite_solutions` pool stores the top `elite_pool_size`\n",
    "        solutions (fitness, mask) found so far. This helps preserve good\n",
    "        solutions and can be used by operators (e.g., MWPA) for\n",
    "        guidance.\n",
    "5.  **Global Updates:**\n",
    "    -   `_aco_pheromone_update()`: Pheromones are updated globally based\n",
    "        on `gbest_mask` and elite solutions. Deposit is proportional to\n",
    "        positive fitness.\n",
    "    -   `_abc_scout_phase()`: Checks agents (especially those modified\n",
    "        by ABC-like ops or stagnated) and resets them if their trial\n",
    "        count exceeds `abc_limit`.\n",
    "6.  **Stagnation Handling & Diversification\n",
    "    (`_handle_stagnation_and_diversify`):**\n",
    "    -   If the global best fitness doesn’t improve for\n",
    "        `stagnation_reset_patience` iterations, a portion of the\n",
    "        population (the worst-performing agents) is reset.\n",
    "    -   Reset positions are either random or based on a mutated elite\n",
    "        solution to inject diversity.\n",
    "7.  **Dynamic Parameters:**\n",
    "    -   PSO’s inertia weight (`pso_w`) and MWPA’s `beta` and `a`\n",
    "        parameters are made dynamic, changing over iterations to balance\n",
    "        exploration and exploitation.\n",
    "8.  **Initialization and Main Loop:**\n",
    "    -   `_initialize_population` sets up initial agent states and\n",
    "        evaluates them.\n",
    "    -   The `run` method orchestrates the main loop, applying operators,\n",
    "        performing global updates, handling stagnation, and managing\n",
    "        early stopping.\n",
    "9.  **Clarity and Modularity:** The code is structured into smaller\n",
    "    methods for better readability and maintenance. Parameters for each\n",
    "    component algorithm are grouped in the `__init__`.\n",
    "10. **Verbose Output:** Improved verbose output to track progress,\n",
    "    including periodic display of operator selection probabilities.\n",
    "11. **Final Solution:** The best solution is taken from `gbest_mask`,\n",
    "    with a check against the elite pool (though `gbest_mask` should\n",
    "    ideally be the best if elites are updated correctly).\n",
    "\n",
    "This hybrid design aims to leverage the strengths of different swarm\n",
    "paradigms through adaptive operator selection, maintain solution quality\n",
    "with an elite pool, and escape local optima via stagnation handling and\n",
    "diversification strategies, all while optimizing for the\n",
    "accuracy-FPR-feature_count objective.\n",
    "\n",
    "``` python\n",
    "# 6. Feature selection experiment\n",
    "import random\n",
    "import traceback # Already imported by user, kept for consistency\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Ensure the global fitness weights are defined (e.g., from step 4)\n",
    "# These are used by the fitness calculation within the FS algorithms.\n",
    "# ACC_WEIGHT = 1.0\n",
    "# FPR_WEIGHT = 0.7 # Emphasizing FPR reduction\n",
    "# FEAT_PENALTY_WEIGHT = 0.02\n",
    "\n",
    "feature_selection_results = {}\n",
    "\n",
    "# Check if data is ready for feature selection (from step 2 and 3)\n",
    "data_ready_for_fs = 'data_loaded' in globals() and data_loaded and \\\n",
    "                    'X_tr' in globals() and X_tr.shape[0] > 0 and \\\n",
    "                    'X_val' in globals() and X_val.shape[0] > 0\n",
    "\n",
    "if data_ready_for_fs:\n",
    "    print(\"\\n--- Running Feature Selection Experiments ---\")\n",
    "    print(f\"Using Fitness Weights: ACC={ACC_WEIGHT}, FPR={FPR_WEIGHT}, FEAT_PENALTY={FEAT_PENALTY_WEIGHT}\")\n",
    "\n",
    "    # Define FS methods to run\n",
    "    # Note: HybridSwarmFeatureSelector is a class, needs instantiation\n",
    "    fs_methods_config = {\n",
    "        \"ACO\": {\"func\": aco_fs, \"params\": {\"n_ants\": 20, \"max_iter\": 30, \"patience\": 7}},\n",
    "        \"PSO\": {\"func\": pso_fs, \"params\": {\"n_particles\": 20, \"max_iter\": 30, \"patience\": 7}},\n",
    "        \"ABC\": {\"func\": abc_fs, \"params\": {\"n_bees\": 20, \"max_iter\": 30, \"limit\": 5, \"patience\": 7}},\n",
    "        \"MWPA\": {\"func\": mwpa_fs, \"params\": {\"n_wolves\": 20, \"max_iter\": 30, \"patience\": 7}},\n",
    "        \"Hybrid\": {\n",
    "            \"class\": HybridSwarmFeatureSelector, # It's a class\n",
    "            \"params\": {\n",
    "                \"n_features\": X_tr.shape[1], # Must be passed to constructor\n",
    "                \"n_agents\": 25, # Slightly more agents for hybrid\n",
    "                \"max_iter\": 40, # Slightly more iterations for hybrid\n",
    "                # Other hybrid-specific params can be set here if defaults are not desired\n",
    "                \"pso_w_range\": (0.4, 0.9), \"pso_c1\": 1.5, \"pso_c2\": 1.5,\n",
    "                \"aco_evap_rate\": 0.1, \"aco_deposit_factor\": 0.8,\n",
    "                \"abc_limit\": 6,\n",
    "                \"mwpa_beta_range\": (0.5, 1.5),\n",
    "                \"adaptive_operator_selection_lr\": 0.15,\n",
    "                \"elite_pool_size\": 5,\n",
    "                \"stagnation_reset_patience\": 8,\n",
    "                \"verbose\": True # Ensure hybrid provides output\n",
    "            },\n",
    "            \"run_params\": {\"patience\": 10} # Params for the .run() method\n",
    "        }\n",
    "    }\n",
    "    # Iteration counts are kept moderate for demonstration. For publication-quality results,\n",
    "    # these (and agent counts) would typically be higher (e.g., max_iter=50-100).\n",
    "    # The original code had max_iter=1, which is too low for any meaningful convergence.\n",
    "\n",
    "    for name, config in tqdm(fs_methods_config.items(), desc=\"Running FS Methods\"):\n",
    "        print(f\"\\nRunning FS method: {name}\")\n",
    "        try:\n",
    "            if \"class\" in config: # For Hybrid (class-based)\n",
    "                selector_class = config[\"class\"]\n",
    "                # Pass n_features explicitly from X_tr.shape[1]\n",
    "                current_params = config[\"params\"].copy()\n",
    "                current_params[\"n_features\"] = X_tr.shape[1] \n",
    "                \n",
    "                selector = selector_class(**current_params)\n",
    "                mask, hist, ct = selector.run(X_tr, y_tr, X_val, y_val, **config.get(\"run_params\", {}))\n",
    "            else: # For individual algorithms (function-based)\n",
    "                method_func = config[\"func\"]\n",
    "                mask, hist, ct = method_func(X_tr, y_tr, X_val, y_val, **config[\"params\"])\n",
    "\n",
    "            num_selected_features = int(np.sum(mask)) if isinstance(mask, np.ndarray) and mask.size > 0 else 0\n",
    "            best_fitness_achieved = hist[-1] if hist else -np.inf\n",
    "            \n",
    "            feature_selection_results[name] = {\n",
    "                'selected_mask': mask,\n",
    "                'num_features': num_selected_features,\n",
    "                'fitness_history': hist,\n",
    "                'time': ct,\n",
    "                'best_fitness': best_fitness_achieved\n",
    "            }\n",
    "            print(f\"{name} completed: Features Selected={num_selected_features}, Best Fitness={best_fitness_achieved:.4f}, Time={ct:.2f}s\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error running {name}: {e}\")\n",
    "            print(traceback.format_exc()) # Print full traceback for debugging\n",
    "            feature_selection_results[name] = {\n",
    "                'selected_mask': np.zeros(X_tr.shape[1] if X_tr.shape[0] > 0 else 0, dtype=int), # Default empty mask\n",
    "                'num_features': 0,\n",
    "                'fitness_history': [],\n",
    "                'time': 0,\n",
    "                'best_fitness': -np.inf,\n",
    "                'error': str(e)\n",
    "            }\n",
    "\n",
    "    print(\"\\n--- Feature Selection Experiments Complete ---\")\n",
    "    print(\"\\nSummary of Feature Selection Results:\")\n",
    "    for method_name, results_data in feature_selection_results.items():\n",
    "        if 'error' in results_data:\n",
    "            print(f\"  {method_name}: ERROR - {results_data['error']}\")\n",
    "        else:\n",
    "            print(f\"  {method_name}: Selected Features={results_data['num_features']}, \"\n",
    "                  f\"Best Fitness={results_data['best_fitness']:.4f}, Time={results_data['time']:.2f}s\")\n",
    "\n",
    "    # Plot fitness convergence curves\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    for method_name, results_data in feature_selection_results.items():\n",
    "        if results_data['fitness_history']:\n",
    "            plt.plot(results_data['fitness_history'], label=f\"{method_name} (Best: {results_data['best_fitness']:.3f})\")\n",
    "    \n",
    "    plt.xlabel(\"Iteration\")\n",
    "    plt.ylabel(\"Best Fitness (Accuracy - w_fpr*FPR - w_feat*FeatRatio)\")\n",
    "    plt.title(\"Feature Selection Convergence Curves\")\n",
    "    plt.legend(loc='best')\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "else:\n",
    "    print(\"\\nSkipping Feature Selection experiments: Prerequisite data (X_tr, X_val) is missing or empty.\")\n",
    "    # Initialize to prevent errors in subsequent cells\n",
    "    feature_selection_results = { \n",
    "        name: {\n",
    "            'selected_mask': np.array([]), 'num_features': 0, \n",
    "            'fitness_history': [], 'time': 0, 'best_fitness': -np.inf, 'error': 'Skipped due to missing data'\n",
    "        } for name in [\"ACO\", \"PSO\", \"ABC\", \"MWPA\", \"Hybrid\"]\n",
    "    }\n",
    "```\n",
    "\n",
    "**Rationale for Step 6 Modifications:** 1. **Configuration Dictionary:**\n",
    "\\* A dictionary `fs_methods_config` is used to define the algorithms,\n",
    "their respective functions or classes, and parameters. This makes it\n",
    "easier to manage and modify experiment settings. \\* The Hybrid algorithm\n",
    "is correctly identified as a class that needs instantiation.\n",
    "`n_features` is now explicitly passed to its constructor using\n",
    "`X_tr.shape[1]`. 2. **Parameter Adjustments:** \\* The `max_iter` for all\n",
    "algorithms (including Hybrid) and agent counts (`n_ants`, `n_particles`,\n",
    "etc.) have been increased from the original `max_iter=1` to more\n",
    "reasonable values (e.g., `max_iter=30-40`, `n_agents=20-25`). This\n",
    "allows the algorithms some chance to converge and demonstrate their\n",
    "capabilities. *For rigorous results, these would need to be even\n",
    "higher.* \\* Specific parameters for the Hybrid algorithm (like\n",
    "`adaptive_operator_selection_lr`, `elite_pool_size`, etc.) are set, and\n",
    "`verbose=True` is ensured for the Hybrid method to get detailed output\n",
    "during its run. 3. **Error Handling:** \\* Added\n",
    "`print(traceback.format_exc())` in the `except` block to provide more\n",
    "detailed error information if an FS method fails. \\* If an error occurs,\n",
    "a default empty mask (all zeros) is stored for `selected_mask` to\n",
    "prevent downstream errors. 4. **Results Storage:** \\* Ensured\n",
    "`num_selected_features` is correctly calculated even if `mask` is empty\n",
    "or not an ndarray. \\* `best_fitness_achieved` is safely extracted from\n",
    "`hist`. 5. **Plotting:** \\* The plot label now includes the best fitness\n",
    "achieved by each method for quick comparison on the graph. \\* The Y-axis\n",
    "label is made more descriptive of the composite fitness being optimized.\n",
    "\\* `plt.tight_layout()` is added for better plot appearance. 6.\n",
    "**Clarity and Information:** \\* Prints the fitness weights being used at\n",
    "the start of the experiment. \\* The summary printout is slightly more\n",
    "detailed. 7. **Data Check:** The initial check `data_ready_for_fs` is\n",
    "more robust. 8. **Fallback:** If experiments are skipped,\n",
    "`feature_selection_results` is initialized with error states for all\n",
    "methods to allow subsequent cells (like model training) to gracefully\n",
    "handle this.\n",
    "\n",
    "These changes make the feature selection experiment more robust,\n",
    "configurable, and provide more meaningful (though still potentially\n",
    "preliminary due to iteration limits) results for comparison.\n",
    "\n",
    "``` python\n",
    "# 7. Model training\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "# from sklearn.svm import SVC # Example, can be uncommented if desired\n",
    "# from sklearn.neural_network import MLPClassifier # Example\n",
    "from sklearn.model_selection import RandomizedSearchCV, StratifiedKFold\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "\n",
    "final_models = {}\n",
    "final_model_training_times = {} # Renamed for clarity\n",
    "final_model_best_params_map = {} # Renamed for clarity\n",
    "\n",
    "# Proceed only if feature selection results are available and test data (X_test_p) exists\n",
    "fs_results_available = 'feature_selection_results' in globals() and \\\n",
    "                       any(res.get('selected_mask') is not None and (isinstance(res.get('selected_mask'), np.ndarray) and res.get('selected_mask').size > 0)\n",
    "                           for res in feature_selection_results.values() if 'error' not in res)\n",
    "\n",
    "data_available_for_training = 'X_test_p' in globals() and X_test_p.shape[0] > 0 and \\\n",
    "                                'X_tr' in globals() and 'X_val' in globals() and \\\n",
    "                                X_tr.shape[0] > 0 and X_val.shape[0] > 0\n",
    "\n",
    "\n",
    "if fs_results_available and data_available_for_training:\n",
    "    print(\"\\n--- Starting Final Model Training ---\")\n",
    "\n",
    "    # Define classifiers to train\n",
    "    # RandomForest is a strong baseline. Others can be added.\n",
    "    classifiers_to_train = {\n",
    "        \"RandomForest\": RandomForestClassifier(random_state=42, class_weight='balanced') # Added class_weight for imbalance\n",
    "    }\n",
    "\n",
    "    # Hyperparameter distributions for RandomizedSearchCV\n",
    "    # Tailor these to the specific classifiers being used.\n",
    "    param_grid_map = {\n",
    "        \"RandomForest\": {\n",
    "            'n_estimators': [50, 100, 150], # Reduced for speed; ideally [100, 200, 300]\n",
    "            'max_depth': [10, 20, None], # None means nodes expand until all leaves are pure\n",
    "            'min_samples_split': [2, 5, 10],\n",
    "            'min_samples_leaf': [1, 2, 4],\n",
    "            'bootstrap': [True, False], # Whether bootstrap samples are used\n",
    "            'criterion': ['gini', 'entropy'] # Function to measure quality of a split\n",
    "        }\n",
    "        # \"SVM\": { 'C': [0.1, 1, 10], 'gamma': ['scale', 'auto'], 'kernel': ['rbf', 'linear'] },\n",
    "        # \"NeuralNetwork\": { 'hidden_layer_sizes': [(50,), (100,), (50,25)], 'activation': ['relu', 'tanh'], 'solver': ['adam'], 'alpha': [0.0001, 0.001] }\n",
    "    }\n",
    "\n",
    "    # Combine the internal training (X_tr) and validation (X_val) sets for final model training.\n",
    "    # This uses more data for training the model that will be evaluated on the unseen X_test_p.\n",
    "    if X_tr.size > 0 and X_val.size > 0:\n",
    "        X_train_final = np.vstack((X_tr, X_val))\n",
    "        y_train_final = pd.concat([y_tr, y_val], ignore_index=True)\n",
    "        print(f\"Combined training data for final models: X_train_final shape {X_train_final.shape}, y_train_final shape {y_train_final.shape}\")\n",
    "    else:\n",
    "        print(\"Error: X_tr or X_val is empty. Cannot proceed with model training.\")\n",
    "        # Initialize to prevent errors in subsequent cells\n",
    "        final_models = {fs_name: {clf_name: \"Data Error\" for clf_name in classifiers_to_train} for fs_name in feature_selection_results}\n",
    "        X_train_final, y_train_final = None, None # Mark as None\n",
    "\n",
    "    overall_training_start_time = time.time()\n",
    "\n",
    "    if X_train_final is not None and y_train_final is not None:\n",
    "      # Loop over each feature selection method's results\n",
    "      for fs_method_name, fs_res in tqdm(feature_selection_results.items(), desc=\"FS Methods Loop\"):\n",
    "          if 'error' in fs_res or fs_res.get('selected_mask') is None or fs_res['selected_mask'].size == 0:\n",
    "              print(f\"\\nSkipping model training for FS method '{fs_method_name}' due to previous error or no mask.\")\n",
    "              final_models[fs_method_name] = {name: \"FS Error/No Mask\" for name in classifiers_to_train}\n",
    "              final_model_training_times[fs_method_name] = {name: 0 for name in classifiers_to_train}\n",
    "              final_model_best_params_map[fs_method_name] = {name: {} for name in classifiers_to_train}\n",
    "              continue\n",
    "\n",
    "          selected_mask = fs_res['selected_mask']\n",
    "          num_selected_features = int(np.sum(selected_mask))\n",
    "\n",
    "          if num_selected_features == 0:\n",
    "              print(f\"\\nSkipping model training for FS method '{fs_method_name}': No features were selected.\")\n",
    "              final_models[fs_method_name] = {name: \"No Features Selected\" for name in classifiers_to_train}\n",
    "              final_model_training_times[fs_method_name] = {name: 0 for name in classifiers_to_train}\n",
    "              final_model_best_params_map[fs_method_name] = {name: {} for name in classifiers_to_train}\n",
    "              continue\n",
    "\n",
    "          print(f\"\\nTraining models for FS method '{fs_method_name}' using {num_selected_features} features...\")\n",
    "\n",
    "          # Apply the selected feature mask to the final training data\n",
    "          feature_indices = np.where(selected_mask == 1)[0]\n",
    "          X_train_selected_final = X_train_final[:, feature_indices]\n",
    "\n",
    "          final_models[fs_method_name] = {}\n",
    "          final_model_training_times[fs_method_name] = {}\n",
    "          final_model_best_params_map[fs_method_name] = {}\n",
    "\n",
    "          # Loop over each classifier defined\n",
    "          for clf_name, clf_template in tqdm(classifiers_to_train.items(), desc=f\"Classifiers ({fs_method_name})\", leave=False):\n",
    "              clf_training_start_time = time.time()\n",
    "              \n",
    "              # Stratified K-Folds for cross-validation in RandomizedSearchCV\n",
    "              # Ensures class proportions are maintained in each fold, important for imbalanced data.\n",
    "              cv_strategy = StratifiedKFold(n_splits=3, shuffle=True, random_state=42)\n",
    "\n",
    "              # Setup RandomizedSearchCV for hyperparameter tuning\n",
    "              # n_iter controls how many parameter settings are sampled.\n",
    "              # cv is the cross-validation strategy.\n",
    "              # n_jobs=-1 uses all available CPU cores.\n",
    "              random_search_cv = RandomizedSearchCV(\n",
    "                  estimator=clf_template,\n",
    "                  param_distributions=param_grid_map[clf_name],\n",
    "                  n_iter=5,  # Reduced for speed; ideally 10-20+\n",
    "                  cv=cv_strategy,\n",
    "                  verbose=0, # Reduced verbosity; set to 1 or 2 for more details\n",
    "                  n_jobs=-1,\n",
    "                  random_state=42,\n",
    "                  scoring='f1_weighted' # Score by F1 to balance precision/recall, esp. for imbalance\n",
    "              )\n",
    "\n",
    "              try:\n",
    "                  print(f\"  Tuning {clf_name} for {fs_method_name}...\")\n",
    "                  random_search_cv.fit(X_train_selected_final, y_train_final)\n",
    "                  \n",
    "                  best_model_found = random_search_cv.best_estimator_\n",
    "                  best_params_found = random_search_cv.best_params_\n",
    "                  training_duration = time.time() - clf_training_start_time\n",
    "\n",
    "                  final_models[fs_method_name][clf_name] = best_model_found\n",
    "                  final_model_training_times[fs_method_name][clf_name] = training_duration\n",
    "                  final_model_best_params_map[fs_method_name][clf_name] = best_params_found\n",
    "                  \n",
    "                  print(f\"    >> Tuned {clf_name} for {fs_method_name} in {training_duration:.2f}s. Best F1 (CV): {random_search_cv.best_score_:.4f}\")\n",
    "                  # print(f\"       Best params: {best_params_found}\")\n",
    "\n",
    "\n",
    "              except Exception as e:\n",
    "                  training_duration = time.time() - clf_training_start_time\n",
    "                  print(f\"    Error training '{clf_name}' for '{fs_method_name}': {e}\")\n",
    "                  print(traceback.format_exc())\n",
    "                  final_models[fs_method_name][clf_name] = f\"Training Failed: {e}\"\n",
    "                  final_model_training_times[fs_method_name][clf_name] = training_duration\n",
    "                  final_model_best_params_map[fs_method_name][clf_name] = {}\n",
    "      \n",
    "      overall_training_duration = time.time() - overall_training_start_time\n",
    "      print(f\"\\n--- Final Model Training Complete in {overall_training_duration:.2f}s ---\")\n",
    "\n",
    "else:\n",
    "    print(\"\\nSkipping final model training: Feature selection results or necessary data are not available/valid.\")\n",
    "    # Initialize to prevent errors in subsequent cells\n",
    "    final_models = {fs_name: {clf_name: \"Skipped\" for clf_name in [\"RandomForest\"]} for fs_name in feature_selection_results if feature_selection_results}\n",
    "    final_model_training_times = {fs_name: {clf_name: 0 for clf_name in [\"RandomForest\"]} for fs_name in feature_selection_results if feature_selection_results}\n",
    "    final_model_best_params_map = {fs_name: {clf_name: {} for clf_name in [\"RandomForest\"]} for fs_name in feature_selection_results if feature_selection_results}\n",
    "```\n",
    "\n",
    "**Rationale for Step 7 Modifications:** 1. **Variable Names:** Renamed\n",
    "`final_model_training_time` to `final_model_training_times` and\n",
    "`final_model_best_params` to `final_model_best_params_map` for clarity,\n",
    "as they are dictionaries mapping FS method and classifier names to\n",
    "values. 2. **Data Availability Checks:** Added more robust checks\n",
    "(`fs_results_available`, `data_available_for_training`) at the beginning\n",
    "to ensure that feature selection ran successfully and necessary data\n",
    "partitions (`X_tr`, `X_val`, `X_test_p`) exist before attempting\n",
    "training. 3. **Classifier Configuration:** \\* `classifiers_to_train` and\n",
    "`param_grid_map` make it easy to add/remove classifiers and manage their\n",
    "hyperparameter search spaces. \\* For `RandomForestClassifier`,\n",
    "`class_weight='balanced'` was added. This is beneficial for imbalanced\n",
    "datasets like NSL-KDD, as it adjusts weights inversely proportional to\n",
    "class frequencies. \\* `n_estimators` for RF and `n_iter` for\n",
    "`RandomizedSearchCV` were slightly reduced for faster demonstration\n",
    "runs. For production, these should be higher. 4. **Final Training\n",
    "Data:** Clarified that `X_train_final` and `y_train_final` are created\n",
    "by combining `X_tr, y_tr` and `X_val, y_val`. This uses all available\n",
    "labeled data (except the final test set) for training the tuned models.\n",
    "Added `ignore_index=True` for `pd.concat`. 5. **Error Handling &\n",
    "Skipping:** \\* Improved logic to skip training for an FS method if it\n",
    "previously errored, its mask is missing, or no features were selected.\n",
    "\\* Added `traceback.format_exc()` for more detailed error messages\n",
    "during model training. 6. **Hyperparameter Tuning with\n",
    "`RandomizedSearchCV`:** \\*\n",
    "`cv_strategy = StratifiedKFold(n_splits=3, shuffle=True, random_state=42)`\n",
    "is used for cross-validation. Stratified K-Fold is crucial for\n",
    "imbalanced datasets to ensure each fold has a representative class\n",
    "distribution. `n_splits=3` is a common choice for faster tuning; 5 or 10\n",
    "are also common. \\* `scoring='f1_weighted'` is used in\n",
    "`RandomizedSearchCV`. F1-score is a good metric for imbalanced\n",
    "classification as it balances precision and recall. `f1_weighted`\n",
    "calculates F1 for each label and finds their average weighted by\n",
    "support. \\* Reduced `verbose` for `RandomizedSearchCV` to `0` to keep\n",
    "output cleaner during the loop, but it can be increased for debugging.\n",
    "7. **Output:** More informative print statements, including the best\n",
    "cross-validated F1-score for each tuned model. 8. **Fallback\n",
    "Initialization:** If training is skipped entirely, the result\n",
    "dictionaries are initialized to prevent errors in the evaluation step.\n",
    "\n",
    "These changes aim to make the model training phase more robust, better\n",
    "suited for imbalanced data, and provide clearer feedback on the tuning\n",
    "process.\n",
    "\n",
    "``` python\n",
    "# 8. Model evaluation\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score,\n",
    "    roc_auc_score,\n",
    "    classification_report,\n",
    "    confusion_matrix,\n",
    "    roc_curve,\n",
    "    precision_score,\n",
    "    recall_score,\n",
    "    f1_score\n",
    ")\n",
    "import seaborn as sns # For plotting confusion matrix\n",
    "\n",
    "print(\"\\n--- Starting Final Model Evaluation on Test Set ---\")\n",
    "final_evaluation_results = {}\n",
    "\n",
    "# Check if models are trained and test data is available\n",
    "models_available_for_eval = 'final_models' in globals() and final_models and \\\n",
    "                            any(isinstance(model, RandomForestClassifier) # Check for actual model objects\n",
    "                                for fs_method_models in final_models.values()\n",
    "                                for model in fs_method_models.values())\n",
    "test_data_available_for_eval = 'X_test_p' in globals() and X_test_p.shape[0] > 0 and \\\n",
    "                               'y_test' in globals() and y_test.shape[0] > 0 and \\\n",
    "                               'feature_selection_results' in globals()\n",
    "\n",
    "\n",
    "if models_available_for_eval and test_data_available_for_eval:\n",
    "    # Loop over each feature selection method\n",
    "    for fs_method_name, clf_model_dict in tqdm(final_models.items(), desc=\"FS Methods Evaluation\"):\n",
    "        fs_results_for_method = feature_selection_results.get(fs_method_name, {})\n",
    "        selected_mask = fs_results_for_method.get('selected_mask')\n",
    "\n",
    "        final_evaluation_results[fs_method_name] = {}\n",
    "\n",
    "        # Validate selected_mask\n",
    "        if selected_mask is None or not isinstance(selected_mask, np.ndarray) or selected_mask.size == 0:\n",
    "            print(f\"  Skipping {fs_method_name}: Invalid or missing feature selection mask.\")\n",
    "            final_evaluation_results[fs_method_name] = {\n",
    "                clf_name: \"Invalid/Missing FS Mask\" for clf_name in clf_model_dict.keys()\n",
    "            }\n",
    "            continue\n",
    "        \n",
    "        num_selected_features = int(np.sum(selected_mask))\n",
    "        if num_selected_features == 0:\n",
    "            print(f\"  Skipping {fs_method_name}: No features were selected by this method.\")\n",
    "            final_evaluation_results[fs_method_name] = {\n",
    "                clf_name: \"No Features Selected\" for clf_name in clf_model_dict.keys()\n",
    "            }\n",
    "            continue\n",
    "\n",
    "        # Prepare test set with selected features\n",
    "        # Ensure selected_mask can be applied to X_test_p\n",
    "        if selected_mask.shape[0] != X_test_p.shape[1]:\n",
    "            print(f\"  Skipping {fs_method_name}: Mask shape {selected_mask.shape} incompatible with X_test_p columns {X_test_p.shape[1]}.\")\n",
    "            final_evaluation_results[fs_method_name] = {\n",
    "                clf_name: \"FS Mask Shape Mismatch\" for clf_name in clf_model_dict.keys()\n",
    "            }\n",
    "            continue\n",
    "            \n",
    "        feature_indices = np.where(selected_mask == 1)[0]\n",
    "        X_test_selected = X_test_p[:, feature_indices]\n",
    "\n",
    "        if X_test_selected.shape[1] == 0 and num_selected_features > 0 : # Should not happen if indices are correct\n",
    "             print(f\"  Warning for {fs_method_name}: num_selected_features is {num_selected_features} but X_test_selected has 0 columns.\")\n",
    "             # This indicates an issue with feature_indices or mask application\n",
    "        \n",
    "        # Evaluate each classifier trained for this FS method\n",
    "        for clf_name, model_object in tqdm(\n",
    "            clf_model_dict.items(),\n",
    "            desc=f\"  Classifiers Eval ({fs_method_name})\",\n",
    "            leave=False\n",
    "        ):\n",
    "            # If model training failed or was skipped (model_object is a string)\n",
    "            if not hasattr(model_object, 'predict'): # Check if it's a valid model object\n",
    "                final_evaluation_results[fs_method_name][clf_name] = model_object # Store the error string\n",
    "                print(f\"  Skipping evaluation for {clf_name} under {fs_method_name}: Model not available ({model_object}).\")\n",
    "                continue\n",
    "\n",
    "            try:\n",
    "                # Make predictions on the selected test features\n",
    "                y_pred_test = model_object.predict(X_test_selected)\n",
    "                y_proba_test = model_object.predict_proba(X_test_selected)[:, 1] # Probabilities for the positive class\n",
    "\n",
    "                # --- Core Metrics ---\n",
    "                accuracy = accuracy_score(y_test, y_pred_test)\n",
    "                roc_auc = roc_auc_score(y_test, y_proba_test)\n",
    "                \n",
    "                # Confusion Matrix and False Positive Rate (FPR)\n",
    "                cm = confusion_matrix(y_test, y_pred_test)\n",
    "                tn, fp, fn, tp = cm.ravel() if cm.size == 4 else (0,0,0,0) # Ensure ravel works\n",
    "                \n",
    "                fpr = fp / (fp + tn) if (fp + tn) > 0 else 0.0 # False Positive Rate\n",
    "                fnr = fn / (fn + tp) if (fn + tp) > 0 else 0.0 # False Negative Rate (Miss Rate)\n",
    "\n",
    "                # Detailed classification report (precision, recall, f1-score per class)\n",
    "                # output_dict=True for easier parsing\n",
    "                class_report_dict = classification_report(y_test, y_pred_test, output_dict=True, zero_division=0)\n",
    "\n",
    "                # Store all relevant results\n",
    "                final_evaluation_results[fs_method_name][clf_name] = {\n",
    "                    'accuracy': accuracy,\n",
    "                    'roc_auc': roc_auc,\n",
    "                    'fpr': fpr,\n",
    "                    'fnr': fnr,\n",
    "                    'confusion_matrix': cm.tolist(), # Convert numpy array to list for easier storage/JSON\n",
    "                    'classification_report': class_report_dict,\n",
    "                    'num_features': num_selected_features,\n",
    "                    'y_pred': y_pred_test.tolist(), # Optional: store predictions\n",
    "                    'y_proba': y_proba_test.tolist() # Optional: store probabilities\n",
    "                }\n",
    "\n",
    "                # --- Detailed Output (can be extensive, enable if needed) ---\n",
    "                if True: # Set to False to reduce output\n",
    "                    print(f\"\\nResults for {fs_method_name} + {clf_name}:\")\n",
    "                    print(f\"  Number of Features: {num_selected_features}\")\n",
    "                    print(f\"  Accuracy: {accuracy:.4f}\")\n",
    "                    print(f\"  ROC AUC: {roc_auc:.4f}\")\n",
    "                    print(f\"  False Positive Rate (FPR): {fpr:.4f}\")\n",
    "                    print(f\"  False Negative Rate (FNR): {fnr:.4f}\")\n",
    "                    # print(\"\\n  Classification Report (Test Set):\")\n",
    "                    # print(classification_report(y_test, y_pred_test, zero_division=0))\n",
    "                    # print(\"\\n  Confusion Matrix (Test Set):\")\n",
    "                    # print(cm)\n",
    "\n",
    "                    # Plot Confusion Matrix\n",
    "                    plt.figure(figsize=(6, 4))\n",
    "                    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', cbar=False,\n",
    "                                xticklabels=['Normal', 'Attack'], yticklabels=['Normal', 'Attack'])\n",
    "                    plt.xlabel('Predicted Label')\n",
    "                    plt.ylabel('True Label')\n",
    "                    plt.title(f'CM: {fs_method_name} + {clf_name} ({num_selected_features} feats)')\n",
    "                    plt.tight_layout()\n",
    "                    plt.show()\n",
    "\n",
    "                    # Plot ROC Curve\n",
    "                    # fpr_roc, tpr_roc, _ = roc_curve(y_test, y_proba_test)\n",
    "                    # plt.figure(figsize=(6, 4))\n",
    "                    # plt.plot(fpr_roc, tpr_roc, label=f'{clf_name} (AUC = {roc_auc:.4f})')\n",
    "                    # plt.plot([0, 1], [0, 1], 'k--') # Random guess line\n",
    "                    # plt.xlabel('False Positive Rate')\n",
    "                    # plt.ylabel('True Positive Rate')\n",
    "                    # plt.title(f'ROC Curve: {fs_method_name} + {clf_name}')\n",
    "                    # plt.legend(loc=\"lower right\")\n",
    "                    # plt.grid(True)\n",
    "                    # plt.tight_layout()\n",
    "                    # plt.show()\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"  Error evaluating {clf_name} for {fs_method_name}: {e}\")\n",
    "                print(traceback.format_exc())\n",
    "                final_evaluation_results[fs_method_name][clf_name] = f\"Evaluation Failed: {e}\"\n",
    "    \n",
    "    # --- Summary Table ---\n",
    "    print(\"\\n--- Summary of Test Set Evaluation Results ---\")\n",
    "    summary_list = []\n",
    "    for fs_name_key, clf_results_dict in final_evaluation_results.items():\n",
    "        for clf_name_key, metrics_dict in clf_results_dict.items():\n",
    "            row = {\n",
    "                'FS Method': fs_name_key,\n",
    "                'Classifier': clf_name_key,\n",
    "                'Num Features': metrics_dict.get('num_features', fs_results_for_method.get('num_features', 0) if fs_results_for_method else 0)\n",
    "            }\n",
    "            if isinstance(metrics_dict, str): # Error or skipped message\n",
    "                row.update({'Status': metrics_dict, 'Accuracy': np.nan, 'FPR': np.nan, 'FNR': np.nan, 'ROC AUC': np.nan,\n",
    "                            'Precision (Attack)': np.nan, 'Recall (Attack)': np.nan, 'F1-Score (Attack)': np.nan})\n",
    "            else:\n",
    "                row.update({\n",
    "                    'Status': 'Success',\n",
    "                    'Accuracy': metrics_dict.get('accuracy', np.nan),\n",
    "                    'FPR': metrics_dict.get('fpr', np.nan),\n",
    "                    'FNR': metrics_dict.get('fnr', np.nan),\n",
    "                    'ROC AUC': metrics_dict.get('roc_auc', np.nan),\n",
    "                    'Precision (Attack)': metrics_dict.get('classification_report', {}).get('1', {}).get('precision', np.nan),\n",
    "                    'Recall (Attack)': metrics_dict.get('classification_report', {}).get('1', {}).get('recall', np.nan),\n",
    "                    'F1-Score (Attack)': metrics_dict.get('classification_report', {}).get('1', {}).get('f1-score', np.nan)\n",
    "                })\n",
    "            \n",
    "            # Add FS time and Model Training time\n",
    "            fs_time = feature_selection_results.get(fs_name_key, {}).get('time', np.nan)\n",
    "            model_train_time = final_model_training_times.get(fs_name_key, {}).get(clf_name_key, np.nan)\n",
    "            row['FS Time (s)'] = fs_time\n",
    "            row['Train Time (s)'] = model_train_time\n",
    "            summary_list.append(row)\n",
    "\n",
    "    summary_df = pd.DataFrame(summary_list)\n",
    "    # Define desired column order for the summary table\n",
    "    column_order = [\n",
    "        'FS Method', 'Classifier', 'Num Features', 'Accuracy', 'FPR', 'FNR', 'ROC AUC',\n",
    "        'Precision (Attack)', 'Recall (Attack)', 'F1-Score (Attack)',\n",
    "        'FS Time (s)', 'Train Time (s)', 'Status'\n",
    "    ]\n",
    "    summary_df = summary_df[column_order]\n",
    "    # Sort by a key metric, e.g., Accuracy (desc) then FPR (asc)\n",
    "    summary_df = summary_df.sort_values(by=['Accuracy', 'FPR'], ascending=[False, True])\n",
    "    \n",
    "    print(summary_df.to_string(index=False))\n",
    "    \n",
    "    # Highlight the best performing method based on your criteria (e.g., highest Acc, lowest FPR)\n",
    "    # This is a simple way to highlight; more sophisticated ranking could be done.\n",
    "    if not summary_df.empty and 'Hybrid' in summary_df['FS Method'].values:\n",
    "        hybrid_rf_results = summary_df[(summary_df['FS Method'] == 'Hybrid') & (summary_df['Classifier'] == 'RandomForest') & (summary_df['Status'] == 'Success')]\n",
    "        if not hybrid_rf_results.empty:\n",
    "            best_hybrid_acc = hybrid_rf_results['Accuracy'].max()\n",
    "            best_hybrid_fpr = hybrid_rf_results[hybrid_rf_results['Accuracy'] == best_hybrid_acc]['FPR'].min()\n",
    "            print(f\"\\nBest Hybrid (RandomForest) result: Accuracy={best_hybrid_acc:.4f}, FPR={best_hybrid_fpr:.4f}\")\n",
    "\n",
    "            # Compare with best individual\n",
    "            individual_methods_df = summary_df[~summary_df['FS Method'].isin(['Hybrid', 'No FS']) & (summary_df['Classifier'] == 'RandomForest') & (summary_df['Status'] == 'Success')]\n",
    "            if not individual_methods_df.empty:\n",
    "                best_individual_acc = individual_methods_df['Accuracy'].max()\n",
    "                best_individual_fpr_at_max_acc = individual_methods_df[individual_methods_df['Accuracy'] == best_individual_acc]['FPR'].min()\n",
    "                print(f\"Best Individual method (RandomForest) result: Accuracy={best_individual_acc:.4f}, FPR={best_individual_fpr_at_max_acc:.4f}\")\n",
    "\n",
    "                if best_hybrid_acc > best_individual_acc and best_hybrid_fpr < best_individual_fpr_at_max_acc:\n",
    "                    print(\"\\nCONCLUSION: Hybrid method achieved higher accuracy AND lower FPR than the best individual method.\")\n",
    "                elif best_hybrid_acc > best_individual_acc:\n",
    "                     print(\"\\nCONCLUSION: Hybrid method achieved higher accuracy than the best individual method.\")\n",
    "                elif best_hybrid_fpr < best_individual_fpr_at_max_acc :\n",
    "                     print(\"\\nCONCLUSION: Hybrid method achieved lower FPR than the best individual method (at its max accuracy).\")\n",
    "                else:\n",
    "                    print(\"\\nCONCLUSION: Hybrid method did not clearly outperform the best individual method on both accuracy and FPR.\")\n",
    "            else:\n",
    "                print(\"\\nNo successful individual method results to compare against Hybrid.\")\n",
    "        else:\n",
    "            print(\"\\nHybrid (RandomForest) method did not produce successful results for comparison.\")\n",
    "    else:\n",
    "        print(\"\\nSummary DataFrame is empty or Hybrid results are missing, cannot make a conclusive statement.\")\n",
    "\n",
    "\n",
    "else:\n",
    "    print(\"\\nSkipping final model evaluation: Trained models or test data are not available/valid.\")\n",
    "    summary_df = pd.DataFrame() # Ensure summary_df exists as empty\n",
    "```\n",
    "\n",
    "**Rationale for Step 8 Modifications:** 1. **Robustness Checks:** \\*\n",
    "Added more comprehensive checks (`models_available_for_eval`,\n",
    "`test_data_available_for_eval`) at the beginning to ensure that models\n",
    "were actually trained and test data is ready. \\* Improved validation of\n",
    "`selected_mask` for each FS method, including checks for `None`, type,\n",
    "size, and compatibility with `X_test_p`‘s shape. \\* Ensured\n",
    "`model_object` is a valid model (has `predict` method) before attempting\n",
    "evaluation. 2. **Metrics Calculation:** \\* **FPR and FNR:** Explicitly\n",
    "calculated False Positive Rate (`fpr`) and False Negative Rate (`fnr`)\n",
    "from the confusion matrix (`tn, fp, fn, tp = cm.ravel()`). Added a\n",
    "safeguard for `cm.ravel()` if `cm` is not 2x2. \\* **Storage:** Confusion\n",
    "matrix is converted to a list (`cm.tolist()`) for easier storage (e.g.,\n",
    "if results are saved to JSON). Predictions and probabilities are also\n",
    "optionally stored as lists. 3. **Output and Visualization:** \\* The\n",
    "detailed print output within the loop is made conditional (`if True:`)\n",
    "for easier toggling. \\* **Confusion Matrix Plot:** Added plotting of the\n",
    "confusion matrix using `seaborn.heatmap` for a clearer visual\n",
    "representation of model performance for each FS method + classifier\n",
    "combination. \\* **ROC Curve Plot:** The ROC curve plotting was kept\n",
    "(commented out by default in the detailed output section to avoid too\n",
    "many plots during a full run, but can be enabled). 4. **Summary Table:**\n",
    "\\* The construction of `summary_df` is made more robust, correctly\n",
    "fetching `num_features` from `metrics_dict` or falling back to\n",
    "`fs_results_for_method`. \\* Added False Negative Rate (FNR) to the\n",
    "summary table. \\* Included ’FS Time (s)’ and ‘Train Time (s)’ in the\n",
    "summary table for a more complete overview of costs. \\* Defined a\n",
    "`column_order` for the `summary_df` to ensure a consistent and logical\n",
    "presentation of metrics. \\* The table is sorted by Accuracy (descending)\n",
    "and then FPR (ascending) to easily identify top-performing combinations.\n",
    "5. **Conclusion Drawing:** \\* Added a section at the end to\n",
    "programmatically attempt to draw a conclusion about the Hybrid method’s\n",
    "performance (specifically for RandomForest) compared to the best\n",
    "individual method, focusing on accuracy and FPR. This directly addresses\n",
    "the user’s primary goal. 6. **Error Handling:** Includes\n",
    "`traceback.format_exc()` for detailed error reporting during evaluation.\n",
    "\n",
    "These changes enhance the evaluation process by making it more robust,\n",
    "providing clearer and more comprehensive metrics (especially FPR and\n",
    "FNR), improving visualizations, and attempting to automate the\n",
    "comparison to highlight whether the hybrid approach meets the desired\n",
    "criteria of higher accuracy and lower false positives.\n",
    "\n",
    "``` python\n",
    "# 9. Implement hybrid swarm optimizer for benchmarks\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "# ----------------------------\n",
    "# Benchmark Functions (to minimize) - Standard definitions\n",
    "# ----------------------------\n",
    "\n",
    "def sphere_func(x_vector):\n",
    "    \"\"\"Sphere function: f(x) = sum(x_i^2). Global minimum 0 at x = [0, ..., 0].\"\"\"\n",
    "    return np.sum(np.square(x_vector))\n",
    "\n",
    "def rastrigin_func(x_vector):\n",
    "    \"\"\"Rastrigin function: f(x) = 10*n + sum(x_i^2 - 10*cos(2*pi*x_i)). Min 0 at x=[0,...,0].\"\"\"\n",
    "    n_dim = x_vector.size\n",
    "    return 10 * n_dim + np.sum(x_vector**2 - 10 * np.cos(2 * np.pi * x_vector))\n",
    "\n",
    "def rosenbrock_func(x_vector):\n",
    "    \"\"\"Rosenbrock function: f(x) = sum(100*(x_{i+1} - x_i^2)^2 + (x_i - 1)^2). Min 0 at x=[1,...,1].\"\"\"\n",
    "    if x_vector.size < 2:\n",
    "        return np.sum((x_vector - 1)**2) # Simplified for 1D\n",
    "    return np.sum(100.0 * (x_vector[1:] - x_vector[:-1]**2.0)**2.0 + (x_vector[:-1] - 1)**2.0)\n",
    "\n",
    "# ----------------------------\n",
    "# Example Bounds for Benchmarks\n",
    "# ----------------------------\n",
    "# n_dimensions_benchmark = 10 # Define this before running, e.g., in the next cell or globally\n",
    "# bounds_sphere      = np.array([[-5.12,  5.12]] * n_dimensions_benchmark)\n",
    "# bounds_rastrigin   = np.array([[-5.12,  5.12]] * n_dimensions_benchmark)\n",
    "# bounds_rosenbrock  = np.array([[-2.048, 2.048]] * n_dimensions_benchmark) # Rosenbrock often uses [-5, 10] or similar\n",
    "\n",
    "# ----------------------------\n",
    "# Hybrid Swarm Optimizer Class for Continuous Benchmarks\n",
    "# ----------------------------\n",
    "\n",
    "class HybridContinuousOptimizer:\n",
    "    def __init__(self,\n",
    "                 objective_function,\n",
    "                 bounds_matrix, # Expects numpy array of shape (n_dimensions, 2)\n",
    "                 n_agents=30,\n",
    "                 max_iterations=100,\n",
    "                 # PSO parameters\n",
    "                 pso_w_range=(0.4, 0.9), pso_c1=1.5, pso_c2=1.5,\n",
    "                 # DE parameters (Differential Evolution, a good alternative/addition for continuous)\n",
    "                 de_cr=0.9, de_f=0.5, # Crossover rate, Differential weight\n",
    "                 # Simple random walk / local search component\n",
    "                 local_search_std_dev_factor=0.01, # Factor of domain range for local search\n",
    "                 # Operator probabilities [PSO, DE, LocalSearch]\n",
    "                 operator_probabilities=None,\n",
    "                 adaptive_lr=0.1, # Learning rate for operator probabilities\n",
    "                 verbose=False):\n",
    "        \n",
    "        self.objective_func = objective_function\n",
    "        self.bounds = bounds_matrix \n",
    "        self.n_dimensions = self.bounds.shape[0]\n",
    "        self.domain_range = self.bounds[:, 1] - self.bounds[:, 0]\n",
    "\n",
    "        self.n_agents = n_agents\n",
    "        self.max_iter = max_iterations\n",
    "        self.verbose = verbose\n",
    "\n",
    "        # PSO params\n",
    "        self.pso_w_min, self.pso_w_max = pso_w_range\n",
    "        self.pso_c1, self.pso_c2 = pso_c1, pso_c2\n",
    "        \n",
    "        # DE params\n",
    "        self.de_cr, self.de_f = de_cr, de_f\n",
    "\n",
    "        # Local Search param\n",
    "        self.local_search_std_dev = local_search_std_dev_factor * self.domain_range\n",
    "\n",
    "        # Operators & Adaptive Selection\n",
    "        self.operators = ['PSO', 'DE', 'LocalSearch']\n",
    "        if operator_probabilities is None or len(operator_probabilities) != len(self.operators):\n",
    "            self.op_probs = np.full(len(self.operators), 1.0 / len(self.operators))\n",
    "        else:\n",
    "            self.op_probs = np.array(operator_probabilities, dtype=float)\n",
    "        self.op_rewards = np.zeros(len(self.operators))\n",
    "        self.op_counts = np.zeros(len(self.operators)) + 1e-6 # Avoid division by zero\n",
    "        self.adaptive_lr = adaptive_lr\n",
    "\n",
    "        # Agent states\n",
    "        self.positions = np.zeros((self.n_agents, self.n_dimensions))\n",
    "        self.velocities = np.zeros_like(self.positions) # For PSO\n",
    "        self.fitness_values = np.full(self.n_agents, np.inf)\n",
    "\n",
    "        # Personal bests (for PSO-like behavior if needed, or general agent memory)\n",
    "        self.pbest_positions = np.zeros_like(self.positions)\n",
    "        self.pbest_fitness = np.full(self.n_agents, np.inf)\n",
    "\n",
    "        # Global best\n",
    "        self.gbest_position = np.zeros(self.n_dimensions)\n",
    "        self.gbest_fitness = np.inf\n",
    "        \n",
    "        self.convergence_history = []\n",
    "        self.computation_time = 0.0\n",
    "\n",
    "        if self.verbose:\n",
    "            print(f\"HybridContinuousOptimizer initialized for '{objective_function.__name__}' with {self.n_dimensions} dimensions.\")\n",
    "            print(f\"Operators: {self.operators}, Initial Probs: {self.op_probs}\")\n",
    "\n",
    "    def _initialize_agents(self):\n",
    "        # Initialize positions uniformly within bounds\n",
    "        for i in range(self.n_dimensions):\n",
    "            self.positions[:, i] = np.random.uniform(self.bounds[i, 0], self.bounds[i, 1], self.n_agents)\n",
    "        \n",
    "        self.velocities = np.random.uniform(-0.1 * self.domain_range, 0.1 * self.domain_range, \n",
    "                                            (self.n_agents, self.n_dimensions)) # Scaled initial velocities\n",
    "\n",
    "        for i in range(self.n_agents):\n",
    "            self.fitness_values[i] = self.objective_func(self.positions[i])\n",
    "            if self.fitness_values[i] < self.pbest_fitness[i]:\n",
    "                self.pbest_fitness[i] = self.fitness_values[i]\n",
    "                self.pbest_positions[i] = self.positions[i].copy()\n",
    "            if self.fitness_values[i] < self.gbest_fitness:\n",
    "                self.gbest_fitness = self.fitness_values[i]\n",
    "                self.gbest_position = self.positions[i].copy()\n",
    "        self.convergence_history.append(self.gbest_fitness)\n",
    "\n",
    "    def _select_operator(self):\n",
    "        idx = np.random.choice(len(self.operators), p=self.op_probs)\n",
    "        return idx, self.operators[idx]\n",
    "\n",
    "    def _update_operator_probs(self, op_idx, reward):\n",
    "        self.op_rewards[op_idx] += reward\n",
    "        self.op_counts[op_idx] += 1\n",
    "        avg_rewards = self.op_rewards / self.op_counts\n",
    "        # Softmax-like update for probabilities\n",
    "        exp_rewards = np.exp(self.adaptive_lr * (avg_rewards - np.max(avg_rewards))) # Stability\n",
    "        self.op_probs = exp_rewards / np.sum(exp_rewards)\n",
    "\n",
    "\n",
    "    def _apply_operator_and_evaluate(self, agent_idx, op_name, iteration):\n",
    "        current_pos = self.positions[agent_idx].copy()\n",
    "        candidate_pos = current_pos.copy()\n",
    "        \n",
    "        pso_w = self.pso_w_max - (self.pso_w_max - self.pso_w_min) * (iteration / self.max_iter)\n",
    "\n",
    "        if op_name == 'PSO':\n",
    "            r1, r2 = np.random.rand(self.n_dimensions), np.random.rand(self.n_dimensions)\n",
    "            self.velocities[agent_idx] = (pso_w * self.velocities[agent_idx] +\n",
    "                                          self.pso_c1 * r1 * (self.pbest_positions[agent_idx] - current_pos) +\n",
    "                                          self.pso_c2 * r2 * (self.gbest_position - current_pos))\n",
    "            candidate_pos = current_pos + self.velocities[agent_idx]\n",
    "\n",
    "        elif op_name == 'DE': # Differential Evolution (rand/1/bin variant)\n",
    "            idxs = [idx for idx in range(self.n_agents) if idx != agent_idx]\n",
    "            a, b, c = self.positions[np.random.choice(idxs, 3, replace=False)]\n",
    "            mutant_pos = a + self.de_f * (b - c)\n",
    "            # Binomial Crossover\n",
    "            cross_points = np.random.rand(self.n_dimensions) < self.de_cr\n",
    "            if not np.any(cross_points): # Ensure at least one point from mutant\n",
    "                cross_points[np.random.randint(0, self.n_dimensions)] = True\n",
    "            candidate_pos = np.where(cross_points, mutant_pos, current_pos)\n",
    "\n",
    "        elif op_name == 'LocalSearch': # Simple Gaussian Perturbation\n",
    "            perturbation = np.random.normal(0, self.local_search_std_dev, self.n_dimensions)\n",
    "            candidate_pos = current_pos + perturbation\n",
    "        \n",
    "        # Boundary enforcement (clipping)\n",
    "        candidate_pos = np.clip(candidate_pos, self.bounds[:, 0], self.bounds[:, 1])\n",
    "        \n",
    "        candidate_fitness = self.objective_func(candidate_pos)\n",
    "        original_fitness = self.fitness_values[agent_idx]\n",
    "        reward = 0\n",
    "\n",
    "        if candidate_fitness < self.fitness_values[agent_idx]:\n",
    "            self.positions[agent_idx] = candidate_pos\n",
    "            self.fitness_values[agent_idx] = candidate_fitness\n",
    "            reward = original_fitness - candidate_fitness # Positive reward for improvement\n",
    "\n",
    "            if candidate_fitness < self.pbest_fitness[agent_idx]:\n",
    "                self.pbest_fitness[agent_idx] = candidate_fitness\n",
    "                self.pbest_positions[agent_idx] = candidate_pos.copy()\n",
    "            \n",
    "            if candidate_fitness < self.gbest_fitness:\n",
    "                self.gbest_fitness = candidate_fitness\n",
    "                self.gbest_position = candidate_pos.copy()\n",
    "        return reward\n",
    "\n",
    "    def run(self, patience=15):\n",
    "        start_time = time.time()\n",
    "        self._initialize_agents()\n",
    "        \n",
    "        stall_iterations = 0\n",
    "        last_best_fitness_val = self.gbest_fitness\n",
    "\n",
    "        for iteration in range(self.max_iter):\n",
    "            for i in range(self.n_agents):\n",
    "                op_idx, op_name = self._select_operator()\n",
    "                reward = self._apply_operator_and_evaluate(i, op_name, iteration)\n",
    "                self._update_operator_probs(op_idx, reward)\n",
    "            \n",
    "            self.convergence_history.append(self.gbest_fitness)\n",
    "            \n",
    "            if self.verbose and (iteration % 10 == 0 or iteration == self.max_iter - 1) :\n",
    "                print(f\"Iter {iteration+1}/{self.max_iter}: Best Fitness = {self.gbest_fitness:.4e}, \"\n",
    "                      f\"Op Probs: {[f'{p:.2f}' for p in self.op_probs]}\")\n",
    "\n",
    "            # Early stopping\n",
    "            if self.gbest_fitness < last_best_fitness_val:\n",
    "                last_best_fitness_val = self.gbest_fitness\n",
    "                stall_iterations = 0\n",
    "            else:\n",
    "                stall_iterations += 1\n",
    "            \n",
    "            if stall_iterations >= patience:\n",
    "                if self.verbose: print(f\"Early stopping at iteration {iteration+1} due to stagnation.\")\n",
    "                break\n",
    "                \n",
    "        self.computation_time = time.time() - start_time\n",
    "        if self.verbose:\n",
    "            print(f\"Optimization finished in {self.computation_time:.2f}s. Best fitness: {self.gbest_fitness:.4e}\")\n",
    "        return self.gbest_position, self.gbest_fitness, self.convergence_history, self.computation_time\n",
    "\n",
    "# Example usage (will be in the next cell for actual execution)\n",
    "# n_dimensions_benchmark = 10\n",
    "# bounds_sphere_ex = np.array([[-5.12,  5.12]] * n_dimensions_benchmark)\n",
    "# optimizer = HybridContinuousOptimizer(sphere_func, bounds_sphere_ex, n_agents=30, max_iterations=100, verbose=True)\n",
    "# best_sol, best_val, history, time_taken = optimizer.run()\n",
    "# print(f\"Sphere Best Value: {best_val:.4e}, Time: {time_taken:.2f}s\")\n",
    "```\n",
    "\n",
    "**Rationale for Step 9 Modifications:** 1. **Class Name:** Renamed to\n",
    "`HybridContinuousOptimizer` to clearly distinguish it from the feature\n",
    "selection hybrid optimizer. 2. **Benchmark Function Names:** Changed\n",
    "function names (e.g., `sphere` to `sphere_func`) to avoid potential\n",
    "conflicts if these names are used elsewhere, and to be more descriptive.\n",
    "3. **Operators:** \\* The original benchmark hybrid used\n",
    "`ACO, PSO+MWPA, ABC`. For continuous optimization, Differential\n",
    "Evolution (DE) is a very powerful and common algorithm. \\* The operators\n",
    "are changed to `['PSO', 'DE', 'LocalSearch']`. \\* **PSO:** Standard PSO\n",
    "update. \\* **DE:** A `rand/1/bin` variant of Differential Evolution is\n",
    "implemented, which is a common and effective choice. \\* **LocalSearch:**\n",
    "A simple Gaussian perturbation around the current position to explore\n",
    "the immediate neighborhood. \\* This set of operators provides a good mix\n",
    "of global exploration (DE, PSO) and local exploitation (LocalSearch,\n",
    "PSO). 4. **Adaptive Operator Selection:** \\* The mechanism for selecting\n",
    "operators and updating their probabilities based on rewards is retained\n",
    "and adapted from the feature selection hybrid. This allows the optimizer\n",
    "to learn which operator is performing well for the given problem and\n",
    "phase of the search. 5. **Parameterization:** \\* Parameters for PSO\n",
    "(`pso_w_range`, `pso_c1`, `pso_c2`) and DE (`de_cr`, `de_f`) are exposed\n",
    "in the constructor. \\* `local_search_std_dev_factor` controls the\n",
    "intensity of the local search. 6. **Boundary Handling:** Standard\n",
    "clipping is used to keep solutions within the defined bounds. 7.\n",
    "**Initialization:** \\* Agent positions are initialized uniformly within\n",
    "their bounds. \\* Initial velocities for PSO are scaled by the domain\n",
    "range of each dimension. 8. **Early Stopping:** Basic early stopping\n",
    "based on fitness stagnation is included. 9. **Clarity and Structure:**\n",
    "The code is organized with private helper methods for initialization,\n",
    "operator selection, and application, similar to the feature selection\n",
    "hybrid. 10. **Verbose Output:** Includes iteration number, best fitness,\n",
    "and current operator probabilities if `verbose=True`. 11. **Bounds\n",
    "Input:** The `bounds_matrix` parameter now explicitly expects a NumPy\n",
    "array of shape `(n_dimensions, 2)`.\n",
    "\n",
    "This revised `HybridContinuousOptimizer` is designed to be a more\n",
    "standard and potentially more effective hybrid for general continuous\n",
    "optimization problems by incorporating DE, a strong performer in this\n",
    "domain, along with PSO and a local search component, all managed by an\n",
    "adaptive operator selection strategy.\n",
    "\n",
    "``` python\n",
    "# 10. Benchmark function analysis\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Ensure benchmark functions (sphere_func, rastrigin_func, rosenbrock_func) and\n",
    "# HybridContinuousOptimizer class are defined (from step 9).\n",
    "\n",
    "# --- Define Benchmark Problem Settings ---\n",
    "n_dimensions_benchmark = 10 # Standard dimension for testing\n",
    "\n",
    "# Bounds for each function (as numpy arrays)\n",
    "bounds_sphere = np.array([[-5.12, 5.12]] * n_dimensions_benchmark)\n",
    "bounds_rastrigin = np.array([[-5.12, 5.12]] * n_dimensions_benchmark)\n",
    "# Rosenbrock is often tested in [-5, 10] or [-2.048, 2.048]. Using the latter.\n",
    "bounds_rosenbrock = np.array([[-2.048, 2.048]] * n_dimensions_benchmark)\n",
    "\n",
    "benchmark_problems = {\n",
    "    'Sphere': {'func': sphere_func, 'bounds': bounds_sphere, 'known_min': 0.0},\n",
    "    'Rastrigin': {'func': rastrigin_func, 'bounds': bounds_rastrigin, 'known_min': 0.0},\n",
    "    'Rosenbrock': {'func': rosenbrock_func, 'bounds': bounds_rosenbrock, 'known_min': 0.0}\n",
    "}\n",
    "\n",
    "benchmark_run_results = {} # Stores results for each benchmark function\n",
    "\n",
    "if n_dimensions_benchmark > 0:\n",
    "    print(f\"\\n--- Running Benchmark Function Analysis (HybridContinuousOptimizer, D={n_dimensions_benchmark}) ---\")\n",
    "\n",
    "    # Common parameters for the HybridContinuousOptimizer runs\n",
    "    optimizer_common_params = {\n",
    "        'n_agents': 50,        # Number of agents in the swarm\n",
    "        'max_iterations': 200, # Max iterations per run (increased for better convergence)\n",
    "        # Operator-specific params (can be tuned per benchmark if needed)\n",
    "        'pso_w_range': (0.4, 0.9), 'pso_c1': 1.5, 'pso_c2': 1.5,\n",
    "        'de_cr': 0.9, 'de_f': 0.6,\n",
    "        'local_search_std_dev_factor': 0.01,\n",
    "        'operator_probabilities': [0.4, 0.4, 0.2], # Initial bias: PSO, DE, LocalSearch\n",
    "        'adaptive_lr': 0.1,\n",
    "        'verbose': False # Set to True for detailed output from optimizer, False for cleaner tqdm loop\n",
    "    }\n",
    "    \n",
    "    num_runs_per_benchmark = 3 # Perform multiple runs for robustness (e.g., 5-10 for actual analysis)\n",
    "\n",
    "    for problem_name, config in tqdm(benchmark_problems.items(), desc=\"Running Benchmarks\"):\n",
    "        print(f\"\\nOptimizing {problem_name} function...\")\n",
    "        \n",
    "        all_runs_best_fitness = []\n",
    "        all_runs_histories = []\n",
    "        all_runs_times = []\n",
    "\n",
    "        for run_num in range(num_runs_per_benchmark):\n",
    "            if optimizer_common_params.get('verbose', False): # only print if optimizer itself is verbose\n",
    "                print(f\"  Run {run_num + 1}/{num_runs_per_benchmark} for {problem_name}...\")\n",
    "            \n",
    "            try:\n",
    "                optimizer = HybridContinuousOptimizer(\n",
    "                    objective_function=config['func'],\n",
    "                    bounds_matrix=config['bounds'],\n",
    "                    **optimizer_common_params\n",
    "                )\n",
    "                # Run with patience for early stopping\n",
    "                best_solution, best_fitness, history, time_taken = optimizer.run(patience=25) \n",
    "                \n",
    "                all_runs_best_fitness.append(best_fitness)\n",
    "                all_runs_histories.append(history)\n",
    "                all_runs_times.append(time_taken)\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"  Error during run {run_num + 1} for {problem_name}: {e}\")\n",
    "                print(traceback.format_exc())\n",
    "                all_runs_best_fitness.append(np.inf) # Error case\n",
    "                all_runs_histories.append([np.inf] * optimizer_common_params['max_iterations'])\n",
    "                all_runs_times.append(0)\n",
    "        \n",
    "        # Aggregate results from multiple runs\n",
    "        avg_best_fitness = np.mean(all_runs_best_fitness)\n",
    "        std_best_fitness = np.std(all_runs_best_fitness)\n",
    "        min_best_fitness = np.min(all_runs_best_fitness) # Best fitness found across all runs\n",
    "        avg_time = np.mean(all_runs_times)\n",
    "        \n",
    "        # For plotting, average the convergence histories (pad if lengths differ due to early stopping)\n",
    "        max_len_history = max(len(h) for h in all_runs_histories)\n",
    "        padded_histories = [np.pad(h, (0, max_len_history - len(h)), 'edge') for h in all_runs_histories]\n",
    "        avg_history = np.mean(np.array(padded_histories), axis=0)\n",
    "\n",
    "        benchmark_run_results[problem_name] = {\n",
    "            'avg_best_fitness': avg_best_fitness,\n",
    "            'std_best_fitness': std_best_fitness,\n",
    "            'min_best_fitness': min_best_fitness, # Overall best\n",
    "            'avg_time_s': avg_time,\n",
    "            'avg_convergence_history': avg_history.tolist(), # Store as list\n",
    "            'known_minimum': config['known_min'],\n",
    "            'status': 'Success' if np.isfinite(avg_best_fitness) else 'Failed'\n",
    "        }\n",
    "        print(f\"  {problem_name} completed. Avg Best Fitness: {avg_best_fitness:.4e} (Std: {std_best_fitness:.2e}), \"\n",
    "              f\"Overall Min Fitness: {min_best_fitness:.4e}, Avg Time: {avg_time:.2f}s\")\n",
    "\n",
    "    print(\"\\n--- Benchmark Function Analysis Complete ---\")\n",
    "\n",
    "    # Plot average convergence curves\n",
    "    plt.figure(figsize=(12, 7))\n",
    "    for problem_name, results in benchmark_run_results.items():\n",
    "        if results['avg_convergence_history']:\n",
    "            plt.plot(results['avg_convergence_history'], \n",
    "                     label=f\"{problem_name} (Avg Best: {results['avg_best_fitness']:.2e}, Min: {results['min_best_fitness']:.2e})\")\n",
    "    \n",
    "    plt.xlabel(\"Iteration\")\n",
    "    plt.ylabel(\"Average Best Objective Function Value\")\n",
    "    plt.title(f\"Hybrid Optimizer Convergence on Benchmark Functions (D={n_dimensions_benchmark}, {num_runs_per_benchmark} runs avg)\")\n",
    "    plt.yscale('log') # Log scale is common for benchmark functions\n",
    "    plt.legend(loc='upper right')\n",
    "    plt.grid(True, which=\"both\", ls=\"--\", alpha=0.7)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    # Summarize benchmark results in a table\n",
    "    print(\"\\nBenchmark Summary (HybridContinuousOptimizer):\")\n",
    "    summary_data_list = []\n",
    "    for problem_name, res_dict in benchmark_run_results.items():\n",
    "        summary_data_list.append({\n",
    "            'Function': problem_name,\n",
    "            'Dimensions': n_dimensions_benchmark,\n",
    "            'Known Minimum': res_dict.get('known_minimum', 'N/A'),\n",
    "            'Avg Best Fitness': f\"{res_dict['avg_best_fitness']:.4e}\",\n",
    "            'Std Best Fitness': f\"{res_dict['std_best_fitness']:.2e}\",\n",
    "            'Min Best Fitness': f\"{res_dict['min_best_fitness']:.4e}\", # Overall best from all runs\n",
    "            'Avg Time (s)': f\"{res_dict['avg_time_s']:.2f}\",\n",
    "            'Status': res_dict.get('status', 'N/A')\n",
    "        })\n",
    "    benchmark_summary_df = pd.DataFrame(summary_data_list)\n",
    "    print(benchmark_summary_df.to_string(index=False))\n",
    "\n",
    "else:\n",
    "    print(\"\\nSkipping benchmark function analysis: 'n_dimensions_benchmark' is not defined or is zero.\")\n",
    "    benchmark_run_results = {} # Ensure it exists\n",
    "    benchmark_summary_df = pd.DataFrame() # Ensure it exists\n",
    "```\n",
    "\n",
    "**Rationale for Step 10 Modifications:** 1. **Problem Definitions:** \\*\n",
    "`n_dimensions_benchmark` is clearly defined at the top. \\*\n",
    "`benchmark_problems` dictionary now stores the function, its bounds, and\n",
    "its known global minimum, making the setup cleaner and more extensible.\n",
    "2. **Multiple Runs:** \\* The analysis now performs\n",
    "`num_runs_per_benchmark` (e.g., 3 for quick demo, ideally 10-30 for\n",
    "robust analysis) for each benchmark function. This is crucial because\n",
    "stochastic optimizers can have variance in their results. \\* Results\n",
    "like average best fitness, standard deviation of best fitness, overall\n",
    "minimum fitness, and average computation time are calculated across\n",
    "these runs. 3. **Optimizer Parameters:** \\* `optimizer_common_params`\n",
    "dictionary centralizes settings for `HybridContinuousOptimizer`.\n",
    "`max_iterations` is increased for a better chance of convergence. \\*\n",
    "Initial operator probabilities are slightly biased towards PSO and DE,\n",
    "as they are often strong global searchers for continuous problems. \\*\n",
    "`verbose` for the optimizer itself is set to `False` to keep the `tqdm`\n",
    "loop output clean, but can be enabled for debugging individual optimizer\n",
    "runs. 4. **Convergence History Aggregation:** \\* The convergence\n",
    "histories from multiple runs are averaged for plotting. Since early\n",
    "stopping can lead to histories of different lengths, they are padded to\n",
    "the maximum length observed before averaging. 5. **Plotting:** \\* The\n",
    "plot now shows the *average* convergence curve for each benchmark\n",
    "function. \\* The legend includes both the average best fitness and the\n",
    "overall minimum fitness achieved across runs. \\* The Y-axis is set to a\n",
    "log scale, which is standard for visualizing convergence on benchmark\n",
    "functions where fitness values can span several orders of magnitude. \\*\n",
    "Grid lines are improved for better readability. 6. **Summary Table:** \\*\n",
    "The `benchmark_summary_df` now includes: \\* Number of dimensions. \\*\n",
    "Known global minimum for reference. \\* Average best fitness. \\* Standard\n",
    "deviation of best fitness (indicates consistency). \\* Minimum best\n",
    "fitness (the absolute best result found). \\* Average computation time.\n",
    "7. **Error Handling:** Basic error handling is included for individual\n",
    "optimizer runs within the multiple runs loop. 8. **Clarity:** Print\n",
    "statements and comments are updated for better understanding of the\n",
    "process.\n",
    "\n",
    "These changes make the benchmark analysis more rigorous by incorporating\n",
    "multiple runs, providing statistical summaries of performance, and\n",
    "improving the clarity of visualizations and result tables. This gives a\n",
    "more reliable assessment of the `HybridContinuousOptimizer`’s\n",
    "capabilities."
   ],
   "id": "42a14256-91c8-446e-92a7-ab6bf11055d3"
  }
 ],
 "nbformat": 4,
 "nbformat_minor": 5,
 "metadata": {}
}
